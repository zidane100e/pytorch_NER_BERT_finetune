{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle as pk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "from transformers import BertTokenizer, BertConfig, BertForTokenClassification\n",
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, AutoModelForTokenClassification\n",
    "from transformers import pipeline, AdamW\n",
    "from torchcrf import CRF as tcrf\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import gluonnlp as nlp\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import BertTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count()>1:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "elif torch.cuda.device_count()>0:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/kobert_data_train.pk', 'rb') as f1:\n",
    "    sentences_train, labels_train = pk.load(f1)\n",
    "with open('data/kobert_data_val.pk', 'rb') as f1:\n",
    "    sentences_test, labels_test = pk.load(f1)\n",
    "with open('data/kobert_data_test.pk', 'rb') as f1:\n",
    "    sentences_test2, labels_test2 = pk.load(f1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁삼성전자', '가', '▁', '디스플레이', '▁관련', '▁일', '회', '성', '▁수익', '에', '▁힘입어', '▁시장', '▁예상', '을', '▁큰', '▁폭', '▁뛰어넘', '는', '▁실적', '을', '▁냈다', '▁', '.']\n",
      "['B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "for ii in range(0, 1):\n",
    "    print(sentences_test2[ii])\n",
    "    print(labels_test2[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagmap = {}\n",
    "tag_values = set()\n",
    "for tags_of_sent in labels_train + labels_test:\n",
    "    for tag_of_word in tags_of_sent:\n",
    "        tag1 = tag_of_word\n",
    "        tagmap.setdefault(tag1, 0)\n",
    "        tagmap[tag1] += 1\n",
    "        \n",
    "tag_values = list(set(tagmap))\n",
    "tag_values = sorted(tag_values)\n",
    "tag_values = tag_values + [\"<PAD>\", \"<CLS>\", \"<SEP>\"]\n",
    "#tag_values = tag_values + [\"<PAD>\"]\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "idx2tag = {i: t for i, t in enumerate(tag_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-DAT',\n",
       " 1: 'B-DUR',\n",
       " 2: 'B-LOC',\n",
       " 3: 'B-MNY',\n",
       " 4: 'B-NOH',\n",
       " 5: 'B-ORG',\n",
       " 6: 'B-PER',\n",
       " 7: 'B-PNT',\n",
       " 8: 'B-POH',\n",
       " 9: 'B-TIM',\n",
       " 10: 'I-DAT',\n",
       " 11: 'I-DUR',\n",
       " 12: 'I-LOC',\n",
       " 13: 'I-MNY',\n",
       " 14: 'I-NOH',\n",
       " 15: 'I-ORG',\n",
       " 16: 'I-PER',\n",
       " 17: 'I-PNT',\n",
       " 18: 'I-POH',\n",
       " 19: 'I-TIM',\n",
       " 20: 'O',\n",
       " 21: '<PAD>',\n",
       " 22: '<CLS>',\n",
       " 23: '<SEP>'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 75\n",
    "bs = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "kobert, vocab = get_pytorch_kobert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kobert_Vectorizer():\n",
    "    def __init__(self, vocab, max_len=MAX_LEN):\n",
    "        self.max_len = max_len\n",
    "        self.vocab = vocab\n",
    "        self.tok = get_tokenizer()\n",
    "        self.tokenizer = nlp.data.BERTSPTokenizer(self.tok, vocab, lower=False)\n",
    "        self.tok_transform = nlp.data.BERTSentenceTransform(\n",
    "            self.tokenizer, max_seq_length=MAX_LEN, pad=True, pair=False)\n",
    "        \n",
    "    def __call__(self, sentences, labels):\n",
    "        \"\"\"\n",
    "        sentences : list of list of words\n",
    "        labels : list of list of labels\n",
    "        \"\"\"\n",
    "        ret = {}\n",
    "        ret['input'] = sentences\n",
    "        ret['label'] = labels\n",
    "        \n",
    "        temp = [ self.vectorize_str(sent1) for sent1 in sentences ]\n",
    "        ret['input_ids'] = [x[0] for x in temp]\n",
    "        ret['attention_mask'] = [x[1] for x in temp]\n",
    "        ret['type_id'] = [x[2] for x in temp]\n",
    "        ret['label_ids'] = [[tag2idx[tag1] for tag1 in sent_tag] \n",
    "                            for sent_tag in self.label_pad(ret['label'])]\n",
    "        ret['label_ids'] = torch.LongTensor(ret['label_ids'])\n",
    "        ret['input_ids'] = torch.LongTensor(ret['input_ids'])\n",
    "        ret['attention_mask'] = torch.LongTensor(ret['attention_mask'])\n",
    "        ret['type_id'] = torch.LongTensor(ret['type_id'])\n",
    "        \n",
    "        return ret\n",
    "        \n",
    "    def label_pad(self, labels):\n",
    "        ret = []\n",
    "        for label1 in labels:\n",
    "            if len(label1) < self.max_len-2:\n",
    "                label2 = ['<CLS>'] + label1 + ['<SEP>'] + ['<PAD>']*(self.max_len-2-len(label1))\n",
    "                #label2 = ['<PAD>'] + label1 + ['<PAD>']*(self.max_len-1-len(label1))\n",
    "            else:\n",
    "                label2 = ['<CLS>'] + label1[:self.max_len-2] + ['<SEP>']\n",
    "                #label2 = ['<PAD>'] + label1[:self.max_len-2] + ['<PAD>']\n",
    "            ret.append(label2)\n",
    "        return ret\n",
    "        \n",
    "    def get_attention_mask(self, len_sent, valid_length):\n",
    "        attention_mask = np.zeros(len_sent)\n",
    "        attention_mask[:valid_length] = 1\n",
    "        return attention_mask\n",
    "    \n",
    "    def vectorize_str(self, list_of_words):\n",
    "        input_ids = [ self.vocab.token_to_idx[ix] for ix in list_of_words ]\n",
    "        if len(input_ids) < self.max_len-2:\n",
    "            length = len(input_ids) + 2\n",
    "            input_ids = [2] + input_ids + [3] + [1]*(self.max_len-2-len(input_ids))\n",
    "        else:\n",
    "            length = self.max_len\n",
    "            input_ids = [2] + input_ids[:self.max_len-2] + [3]\n",
    "        #print('input', input_ids)\n",
    "        #print('length', length)\n",
    "        attention_mask = self.get_attention_mask(len(input_ids), length)\n",
    "        type_id = [0] * self.max_len\n",
    "        #print('mask', attention_mask)\n",
    "        return input_ids, attention_mask, type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "vectorizer = Kobert_Vectorizer(vocab)\n",
    "data_train = vectorizer(sentences_train, labels_train)\n",
    "data_test = vectorizer(sentences_test, labels_test)\n",
    "data_test2 = vectorizer(sentences_test2, labels_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input 20 ['▁', '●', '봉', '황', '기', '▁전국', '대회', '▁(', '▁오전', '9', '시', '▁', '·', '▁창원', '종합', '사', '격', '장', '▁', ')']\n",
      "label 20 ['O', 'O', 'B-POH', 'I-POH', 'I-POH', 'I-POH', 'I-POH', 'O', 'B-TIM', 'I-TIM', 'I-TIM', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'O']\n",
      "input_ids 75 tensor([   2,  517,    0, 6392, 7951, 5561, 4014, 5829,  522, 3431,  234, 6705,\n",
      "         517,  478, 4441, 7270, 6493, 5412, 7178,  517,   40,    3,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1])\n",
      "attention_mask 75 tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "type_id 75 tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0])\n",
      "label_ids 75 tensor([22, 20, 20,  8, 18, 18, 18, 18, 20,  9, 19, 19, 20, 20,  2, 12, 12, 12,\n",
      "        12, 20, 20, 23, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21])\n"
     ]
    }
   ],
   "source": [
    "ii = 6\n",
    "#for key, val in data_train.items():\n",
    "#    print(key, len(val[ii]), val[ii])\n",
    "ii = 3\n",
    "for key, val in data_test.items():\n",
    "    print(key, len(val[ii]), val[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset(data_train['input_ids'], \n",
    "                              data_train['attention_mask'],\n",
    "                              data_train['type_id'],\n",
    "                              data_train['label_ids'])\n",
    "dataset_test = TensorDataset(data_test['input_ids'], \n",
    "                              data_test['attention_mask'],\n",
    "                              data_test['type_id'],\n",
    "                              data_test['label_ids'])\n",
    "dataset_test2 = TensorDataset(data_test2['input_ids'], \n",
    "                              data_test2['attention_mask'],\n",
    "                              data_test2['type_id'],\n",
    "                              data_test2['label_ids'])\n",
    "\n",
    "train_sampler = RandomSampler(dataset_train)\n",
    "valid_sampler = SequentialSampler(dataset_test)\n",
    "valid_sampler2 = SequentialSampler(dataset_test2)\n",
    "\n",
    "train_dataloader = DataLoader(dataset_train, sampler=train_sampler, batch_size=bs)\n",
    "valid_dataloader = DataLoader(dataset_test, sampler=valid_sampler, batch_size=bs)\n",
    "valid_dataloader2 = DataLoader(dataset_test2, sampler=valid_sampler2, batch_size=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ninput_ids = [ encoded1['input_ids'] for encoded1 in encoded ]\\nattention_masks = [ encoded1['attention_mask'] for encoded1 in encoded ]\\ntoken_type_ids = [ encoded1['token_type_ids'] for encoded1 in encoded ]\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "input_ids = [ encoded1['input_ids'] for encoded1 in encoded ]\n",
    "attention_masks = [ encoded1['attention_mask'] for encoded1 in encoded ]\n",
    "token_type_ids = [ encoded1['token_type_ids'] for encoded1 in encoded ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoBERT_CRF(nn.Module):\n",
    "    def __init__(self, kobert, n_tag):\n",
    "        super().__init__()\n",
    "        hidden_size = 768\n",
    "        dropout_prob = 0.1\n",
    "        self.kobert = kobert\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.ff = nn.Linear(hidden_size, n_tag)\n",
    "        self.crf = tcrf(len(tag2idx), batch_first=True)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, type_ids):\n",
    "        \"\"\"\n",
    "        hidden : [batch, len_seq, dim_hidden]\n",
    "        \"\"\"\n",
    "        hidden, _, _ = self.kobert(input_ids, attention_mask, type_ids)\n",
    "        hidden = self.dropout(hidden)\n",
    "        emissions = self.ff(hidden)\n",
    "        return emissions\n",
    "    \n",
    "    def neg_log_likelihood(self, input_ids, attention_mask, type_ids, label_ids):\n",
    "        emissions = self.forward(input_ids, attention_mask, type_ids)\n",
    "        \n",
    "        loss = -1*self.crf.forward(emissions, label_ids, \n",
    "                                    mask=attention_mask.to(dtype=torch.uint8),\n",
    "                                    reduction='mean')\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KoBERT_CRF(kobert, len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count()>1:\n",
    "    model = nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,        \n",
    "    lr=3e-5,\n",
    "    eps=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0,\n",
    "    \n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef flat_accuracy(preds, labeltttts):\\n    pred_flat = np.argmax(preds, axis=2).flatten()\\n    labels_flat = labels.flatten()\\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from seqeval.metrics import f1_score\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "def filtered_label(preds, labels, except_ids):\n",
    "    #pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    pred_flat = np.array(preds).flatten()\n",
    "    #print('pred_flat', pred_flat)\n",
    "    labels_flat = labels.flatten()\n",
    "    mask = []\n",
    "    for i, x in enumerate(labels_flat):\n",
    "        if x in except_ids:\n",
    "            mask.append(i)\n",
    "    #mask = np.where(labels_flat==except_ids)\n",
    "    pred_flat = np.delete(pred_flat, mask)\n",
    "    #print('pred_flat2', pred_flat)\n",
    "    labels_flat = np.delete(labels_flat, mask)\n",
    "    return pred_flat, labels_flat\n",
    "\n",
    "def flat_accuracy(preds, labels, except_ids):\n",
    "    except_ids = [tag2idx[tag] for tag in except_ids]\n",
    "    pred_flat, labels_flat = filtered_label(preds, labels, except_ids)\n",
    "    return accuracy_score(labels_flat, pred_flat)\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\"\"\"\n",
    "def flat_accuracy(preds, labeltttts):\n",
    "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 27.294172938664754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|▏         | 1/60 [02:21<2:18:59, 141.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.19860303401947\n",
      "Validation Accuracy: 0.8656259896384498\n",
      "Validation F1-Score: 0.6746000721110412\n"
     ]
    }
   ],
   "source": [
    "## Store the average loss after each epoch so we can plot them.\n",
    "loss_values, validation_loss_values = [], []\n",
    "loss_fct = nn.CrossEntropyLoss(ignore_index = tag2idx['<PAD>'])\n",
    "#eff_labels = list( set(tag2idx.keys()) - set(['<PAD>', '<CLS>', '<SEP>', 'O']) )\n",
    "eff_labels = list( set(tag2idx.keys()) - set(['<PAD>', '<CLS>', '<SEP>', 'O']) )\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    # Perform one full pass over the training set.\n",
    "    \n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_mask_id, b_type_id, b_labels_id = batch\n",
    "        \n",
    "        # Always clear any previously calculated gradients before performing a backward pass.\n",
    "        model.zero_grad()\n",
    "        #\"\"\"       \n",
    "        # forward pass\n",
    "        # This will return the loss (rather than the model output)\n",
    "        # because we have provided the `labels`.\n",
    "        n_batch, len_sent = b_input_ids.size()\n",
    "        logits = model(b_input_ids, b_mask_id, b_type_id)\n",
    "                        #attention_mask=b_mask_id, labels=b_labels_id)\n",
    "        #loss = loss_fct(logits.view(n_batch*len_sent, -1), b_labels_id.view(n_batch*len_sent))\n",
    "        loss = model.neg_log_likelihood(b_input_ids, b_mask_id, \n",
    "                                        b_type_id, b_labels_id)\n",
    "        # get the loss\n",
    "        \n",
    "        #loss = loss.mean()\n",
    "        #print('loss', loss)\n",
    "        #\"\"\"\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "        # track train loss\n",
    "        total_loss += loss.item()    \n",
    "        # Clip the norm of the gradient\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "        \n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "    \n",
    "    \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "    \n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    # Reset the validation loss for this epoch.\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_mask_id, b_type_id, b_labels_id = batch\n",
    "        n_batch, len_sent = b_input_ids.size()\n",
    "        # Telling the model not to compute or store gradients,\n",
    "        # saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            #outputs = model(b_input_ids, token_type_ids=b_types,\n",
    "            #outputs = model(b_input_ids, token_type_ids=None,\n",
    "            #                attention_mask=b_mask_id, labels=b_labels_id)\n",
    "            outputs = model(b_input_ids, b_mask_id, b_type_id)\n",
    "            #output_tags = model.crf.decode(outputs, b_mask_id.to(dtype=torch.uint8))\n",
    "            output_tags = model.crf.decode(outputs)\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            #logits = outputs.detach().cpu().numpy()\n",
    "            label_ids = b_labels_id.to('cpu').numpy()\n",
    "        \n",
    "            # Calculate the accuracy for this batch of test sentences.\n",
    "            #eval_loss = loss_fct(outputs.view(n_batch*len_sent, -1), b_labels_id.view(n_batch*len_sent))\n",
    "            loss = model.neg_log_likelihood(b_input_ids, b_mask_id, \n",
    "                                        b_type_id, b_labels_id)\n",
    "            #print(loss.size())\n",
    "            loss = loss.detach().cpu().numpy()\n",
    "        eval_loss += loss\n",
    "        #eval_accuracy += flat_accuracy(logits, label_ids, len(tag2idx)-1)\n",
    "        #eval_accuracy += flat_accuracy(logits, label_ids, ['<PAD>', '<CLS>', '<SEP>', 'O'])\n",
    "        eval_accuracy += flat_accuracy(output_tags, label_ids, ['<PAD>', '<CLS>', '<SEP>', 'O'])\n",
    "        #eval_accuracy += flat_accuracy(logits, label_ids, ['<PAD>', '<CLS>', '<SEP>'])\n",
    "        #print('logits', logits[0][:10])\n",
    "        #print('output', output_tags[0][:10])\n",
    "        #print('label', label_ids[0][:10])\n",
    "        #predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        predictions.extend(output_tags)\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tag_values[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tag_values[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    print(\"Validation F1-Score: {}\".format(f1_score(valid_tags, pred_tags,\n",
    "                                                    average='macro', labels=eff_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 16.226093292236328\n",
      "Test Accuracy: 0.7776261937244202\n",
      "Test F1-Score: 0.6320725221374415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "    #               Test\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "    \n",
    "    # Put the model into evaluation mode\n",
    "    model.eval()\n",
    "    # Reset the validation loss for this epoch.\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    for batch in valid_dataloader2:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_mask_id, b_type_id, b_labels_id = batch\n",
    "        n_batch, len_sent = b_input_ids.size()\n",
    "        # Telling the model not to compute or store gradients,\n",
    "        # saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have not provided labels.\n",
    "            #outputs = model(b_input_ids, token_type_ids=b_types,\n",
    "            #outputs = model(b_input_ids, token_type_ids=None,\n",
    "            #                attention_mask=b_mask_id, labels=b_labels_id)\n",
    "            outputs = model(b_input_ids, b_mask_id, b_type_id)\n",
    "            #output_tags = model.crf.decode(outputs, b_mask_id.to(dtype=torch.uint8))\n",
    "            output_tags = model.crf.decode(outputs)\n",
    "            \n",
    "            # Move logits and labels to CPU\n",
    "            #logits = outputs.detach().cpu().numpy()\n",
    "            label_ids = b_labels_id.to('cpu').numpy()\n",
    "        \n",
    "            # Calculate the accuracy for this batch of test sentences.\n",
    "            #eval_loss = loss_fct(outputs.view(n_batch*len_sent, -1), b_labels_id.view(n_batch*len_sent))\n",
    "            loss = model.neg_log_likelihood(b_input_ids, b_mask_id, \n",
    "                                        b_type_id, b_labels_id)\n",
    "            #print(loss.size())\n",
    "            loss = loss.detach().cpu().numpy()\n",
    "        eval_loss += loss\n",
    "        #eval_accuracy += flat_accuracy(logits, label_ids, len(tag2idx)-1)\n",
    "        #eval_accuracy += flat_accuracy(logits, label_ids, ['<PAD>', '<CLS>', '<SEP>', 'O'])\n",
    "        eval_accuracy += flat_accuracy(output_tags, label_ids, ['<PAD>', '<CLS>', '<SEP>', 'O'])\n",
    "        #eval_accuracy += flat_accuracy(logits, label_ids, ['<PAD>', '<CLS>', '<SEP>'])\n",
    "        #print('logits', logits[0][:10])\n",
    "        #print('output', output_tags[0][:10])\n",
    "        #print('label', label_ids[0][:10])\n",
    "        #predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        predictions.extend(output_tags)\n",
    "        true_labels.append(label_ids)\n",
    "        \n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Test loss: {}\".format(eval_loss))\n",
    "    print(\"Test Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
    "    pred_tags = [tag_values[p_i] for p in predictions for p_i in p]\n",
    "    valid_tags = [tag_values[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
    "    print(\"Test F1-Score: {}\".format(f1_score(valid_tags, pred_tags,\n",
    "                                                    average='macro', labels=eff_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "importlib.reload(sys.modules['kobert_decode'])\n",
    "from kobert_decode import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderFromNamedEntitySequence(vectorizer.tokenizer, idx2tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22,  5, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
       "        20, 20, 20, 20, 20, 20, 23, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "        21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "        21, 21, 21])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test2['label_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <삼성전자:ORG>가 디스플레이 관련 일회성 수익에 힘입어 시장 예상을 큰 폭 뛰어넘는 실적을 냈다 .\n",
      " <삼성전자:ORG>가 디스플레이 관련 일회성 수익에 힘입어 시장 예상을 큰 폭 뛰어넘는 실적을 냈다 .\n",
      "\n",
      " <삼성전자는:ORG> 올해 <2분기:NOH> 연결 기준 영업이익이 전년 동기 대비 <22 . 73 %:PNT> 증가한 <8조1천억원:MNY>으로 잠정 집계됐다고 <8일:DAT> 공시했다 .\n",
      " <삼성전자는:ORG> 올해 <2분기:NOH> 연결 기준 영업이익이 전년 동기 대비 <22 . 73 %:PNT> 증가한 <8조1천억원:MNY>으로 잠정 집계됐다고 <8일:DAT> 공시했다 .\n",
      "\n",
      " 매출액은 <52조원:MNY>으로 <7 . 36 %:PNT> 줄었다 .\n",
      " 매출액은 <52조원:MNY>으로 <7 . 36 %:PNT> 줄었다 .\n",
      "\n",
      " 이런 실적은 시장 예상치를 크게 뛰어넘는 것이다 .\n",
      " 이런 실적은 시장 예상치를 크게 뛰어넘는 것이다 .\n",
      "\n",
      " <연합인포맥스가:ORG> 최근 <1개월간:DUR> 실적 전망치를 발표한 <15개:NOH> 증권사를 대상으로 컨센서스를 실시한 결과 <삼성전자는:ORG> 올해 <2분기:NOH> <51조118억원의:MNY> 매출과 <6조5천384억원의:MNY> 영업이익을 거뒀을 것으로 관측됐다 .\n",
      " <연합인포맥스가:ORG> 최근 <1개월간:DUR> 실적 전망치를 발표한 <15개:NOH> 증권사를 대상으로 컨센서스를 실시한 결과 <삼성전자는:ORG> 올해 <2분기:NOH> <51조118억원의:MNY> 매출과 <6조5천384억원의:MNY> 영업이익을 거뒀을 것으로 관측됐다 .\n",
      "\n",
      " <삼성전자는:ORG> 올해 <2분기:NOH> 실적에 대해 \" 디스플레이 관련 일회성 수익이 포함돼 있다 \" 고 설명했다 .\n",
      " <삼성전자는:ORG> 올해 <2분기:NOH> 실적에 대해 \" 디스플레이 관련 일회성 수익이 포함돼 있다 \" 고 설명했다 .\n",
      "\n",
      " <삼성전자는:ORG> <2009년:DAT> <7월부터:DAT> 국내 기업 최초로 분기실적 예상치를 공시하고 있다 .\n",
      " <삼성전자는:ORG> <2009년 7월부터:DAT> 국내 기업 최초로 분기실적 예상치를 공시하고 있다 .\n",
      "\n",
      " <LG화학:ORG>과 <GS칼텍스가:ORG> 전기차 업계 파트너들과 손잡고 빅데이터를 활용한 전기차 배터리 특화 서비스 개발에 나선다 .\n",
      " <LG화학:ORG>과 <GS칼텍스가:ORG> 전기차 업계 파트너들과 손잡고 빅데이터를 활용한 전기차 배터리 특화 서비스 개발에 나선다 .\n",
      "\n",
      " <LG화학:ORG>과 <GS칼텍스는:ORG> <7일:DAT> <여의도:LOC> <LG트윈타워:POH>에서 충전 환경 개선 및 신사업 기회 발굴을 위한 업무협약 ( MOU ) 을 체결했다고 밝혔다 .\n",
      " <LG화학:ORG>과 <GS칼텍스는:ORG> <7일:DAT> <여의도 LG트윈타워:LOC>에서 충전 환경 개선 및 신사업 기회 발굴을 위한 업무협약 ( MOU ) 을 체결했다고 밝혔다 .\n",
      "\n",
      " 이번 업무협약은 충전소에서 수집한 전기차 빅데이터를 활용해 다양한 배터리 특화 서비스를 발굴하기 위한 것으로 , <LG화학:ORG>과 <GS칼텍스는:ORG> 먼저 배터리 안전진단 서비스를 개발하기로 했다 .\n",
      " 이번 업무협약은 충전소에서 수집한 전기차 빅데이터를 활용해 다양한 배터리 특화 서비스를 발굴하기 위한 것으로 , <LG화학:ORG>과 <GS칼텍스는:ORG> 먼저 배터리 안전진단 서비스를 개발하기로 했다 .\n",
      "\n",
      " 배터리 안전진단 서비스는 전기차가 <GS칼텍스:ORG> 충전소에서 충전을 진행하는 동안 주행 및 충전 데이터를 클라우드에 저장하고 , <LG화학:ORG> 빅데이터 분석 및 배터리 서비스 알고리즘을 통해 배터리의 현재 상태와 위험성을 확인해 충전기는 물론 운전자의 휴대전화에서도 바로 확인할 수 있는 서비스\n",
      " 배터리 안전진단 서비스는 전기차가 <GS칼텍스:ORG> 충전소에서 충전을 진행하는 동안 주행 및 충전 데이터를 클라우드에 저장하고 , <LG화학:ORG> 빅데이터 분석 및 배터리 서비스 알고리즘을 통해 배터리의 현재 상태와 위험성을 확인해 충전기는 물론 운전자의 휴대전화에서도 바로 확인할 수 있는 서비스\n",
      "\n",
      " <LG화학:ORG>은 <GS칼텍스와:ORG> <2021년까지:DAT> 실증 사업을 완료한 후 국내 서비스 사업을 시작하고 , <2022년부터:DAT> 해외 충전 시장으로 배터리 특화 서비스 사업을 확대할 예정이다 .\n",
      " <LG화학:ORG>은 <GS칼텍스와:ORG> <2021년까지:DAT> 실증 사업을 완료한 후 국내 서비스 사업을 시작하고 , <2022년부터:DAT> 해외 충전 시장으로 배터리 특화 서비스 사업을 확대할 예정이다 .\n",
      "\n",
      " 양사는 또 배터리 안전진단 서비스를 기반으로 배터리 수명을 개선할 수 있는 스마트 충전 및 잔존 수명 예측 등의 신규 서비스도 발굴하기로 했다 .\n",
      " <양:NOH>사는 또 배터리 안전진단 서비스를 기반으로 배터리 수명을 개선할 수 있는 스마트 충전 및 잔존 수명 예측 등의 신규 서비스도 발굴하기로 했다 .\n",
      "\n",
      " 서비스 개발이 완료되면 운전자는 <GS칼텍스:ORG> 전기차 충전소에서 배터리 안전진단 , 퇴화 방지 알고리즘이 적용된 스마트 충전 , 잔존 수명 예측 서비스 등을 원스톱 서비스로 받을 수 있게 된다 .\n",
      " 서비스 개발이 완료되면 운전자는 <GS칼텍스:ORG> 전기차 충전소에서 배터리 안전진단 , 퇴화 방지 알고리즘이 적용된 스마트 충전 , 잔존 수명 예측 서비스 등을 원스톱 서비스로 받을 수 있게 된다 .\n",
      "\n",
      " 이번 업무협약은 글로벌 전기차 배터리 시장 1위인 <LG화학:ORG> , 에너지 · 모빌리티 분야의 경쟁력을 강화하는 <GS칼텍스가:ORG> 함께 배터리에 특화된 서비스를 상품화한다는 데 의미가 있다 .\n",
      " 이번 업무협약은 글로벌 전기차 배터리 시장 <1위:NOH>인 <LG화학:ORG> , 에너지 · 모빌리티 분야의 경쟁력을 강화하는 <GS칼텍스가:ORG> 함께 배터리에 특화된 서비스를 상품화한다는 데 의미가 있다 .\n",
      "\n",
      " <LG화학:ORG>은 <1만7천건:NOH> 이상의 전기차 배터리 특허를 확보하고 있으며 , 지난해 말 기준 전 세계 <350만대:NOH>에 달하는 전기차에 배터리를 공급했다 .\n",
      " <LG화학:ORG>은 <1만7천건:NOH> 이상의 전기차 배터리 특허를 확보하고 있으며 , 지난해 말 기준 전 세계 <350만대:NOH>에 달하는 전기차에 배터리를 공급했다 .\n",
      "\n",
      " 또 이를 바탕으로 장수명 배터리 기술과 수명 평가 역량을 확보해왔다 .\n",
      " 또 이를 바탕으로 장수명 배터리 기술과 수명 평가 역량을 확보해왔다 .\n",
      "\n",
      " <GS칼텍스는:ORG> 현재 전국 <44개소:NOH> 주유소 · 충전소에 <100kW급:NOH> 전기차 충전기를 설치해 운영하고 있다 .\n",
      " <GS칼텍스는:ORG> 현재 전국 <44개소:NOH> 주유소 · 충전소에 <100kW:NOH>급 전기차 충전기를 설치해 운영하고 있다 .\n",
      "\n",
      " 오는 <2022년까지:DAT> <100kW:NOH> 이상 초급속 전기차 충전기를 <160개:NOH> 수준으로 확장하고 차량 관련 서비스도 확대하는 등 주유소를 거점으로 한 전기차 생태계를 확장할 계획이다 .\n",
      " 오는 <2022년까지:DAT> <100kW:NOH> 이상 초급속 전기차 충전기를 <160개:NOH> 수준으로 확장하고 차량 관련 서비스도 확대하는 등 주유소를 거점으로 한 전기차 생태계를 확장할 계획이다 .\n",
      "\n",
      " <NHN:ORG>이 중국 이커머스 사업에서 질주하고 있다 .\n",
      " <NHN:ORG>이 <중국:ORG> 이커머스 사업에서 질주하고 있다 .\n",
      "\n",
      " <NHN:ORG>은 <2020년:DAT> 상반기 <중국:LOC> 이커머스 사업 누적 거래액이 <1천억원:MNY>을 돌파했다고 <7일:DAT> 밝혔다 .\n",
      " <NHN:ORG>은 <2020년:DAT> 상반기 <중국:ORG> 이커머스 사업 누적 거래액이 <1천억원:MNY>을 돌파했다고 <7일:DAT> 밝혔다 .\n",
      "\n",
      " 특히 <중국의:LOC> 상반기 최대 온라인 쇼핑 행사인 ' <618:NOH> 쇼핑 축제 ( <6월 1 ~ 20일:DUR> ) ' 기간에만 총 <200억원의:MNY> 매출이 발생했다고 밝혔다 .\n",
      " 특히 <중국의:ORG> 상반기 최대 온라인 쇼핑 행사인 ' <618 쇼핑 축제:POH> ( <6월 1 ~ 20일:DUR> ) ' 기간에만 총 <200억원의:MNY> 매출이 발생했다고 밝혔다 .\n",
      "\n",
      " 이는 <신종 코로나바이러스 감염증 ( 코로나19 ) 에:POH> 따른 소비 심리 위축과 물류 및 유통 여건 악화 등 커머스 업계의 전반적인 경기 침체 속에서 이뤄낸 유의미한 성과라고 회사 측은 평가했다 .\n",
      " 이는 신종 <코로나바이러스:POH> 감염증 ( <코로나19:POH> ) 에 따른 소비 심리 위축과 물류 및 유통 여건 악화 등 커머스 업계의 전반적인 경기 침체 속에서 이뤄낸 유의미한 성과라고 회사 측은 평가했다 .\n",
      "\n",
      " 그러면서 <618:NOH> 쇼핑 축제와 ' 언택트 ' 를 전략적으로 공략한 데에 따른 결과라고 설명했다 .\n",
      " 그러면서 <618 쇼핑 축제:POH>와 ' 언택트 ' 를 전략적으로 공략한 데에 따른 결과라고 설명했다 .\n",
      "\n",
      " <중국:LOC> 기반 이커머스 사업은 <NHN고도:ORG>의 계열사인 <NHN에이컴메이트:ORG>에서 맡고 있다 .\n",
      " <중국:ORG> 기반 이커머스 사업은 <NHN고도:ORG>의 계열사인 <NHN에이컴메이트:ORG>에서 맡고 있다 .\n",
      "\n",
      " <에이컴메이트:ORG>는 <618:NOH> 쇼핑 축제 기간 최대 거래액을 달성한 <알리바바:ORG>의 B2C 쇼핑몰 ' <티몰:POH> ' 의 공식 파트너사이며 , 이들 중에서도 거래액 상위 <3 %:PNT> 안에 드는 유일한 한국 기업으로 활약을 하고 있다 .\n",
      " <에이컴메이트:ORG>는 <618 쇼핑 축제:DAT> 기간 최대 거래액을 달성한 <알리바바:ORG>의 B2C 쇼핑몰 ' <티몰:POH> ' 의 공식 파트너사이며 , 이들 중에서도 거래액 상위 <3 %:PNT> 안에 드는 유일한 <한국:LOC> 기업으로 활약을 하고 있다 .\n",
      "\n",
      " <에이컴메이트:ORG>를 통해 중국에 진출하고 있는 한국 브랜드는 약 <100여개:NOH>다 .\n",
      " <에이컴메이트:ORG>를 통해 <중국:ORG>에 진출하고 있는 <한국:LOC> 브랜드는 약 <100여개:NOH>다 .\n",
      "\n",
      " 상품 카테고리별로 화장품이 전년 동기 대비 <106 %:PNT> , 건강기능식품과 퍼스널케어가 각각 <75 %:PNT> 와 <566 %:PNT> 매출이 성장한 것으로 나타났다 .\n",
      " 상품 카테고리별로 화장품이 전년 동기 대비 <106 %:PNT> , 건강기능식품과 퍼스널케어가 각각 <75 %:PNT> 와 <566 %:PNT> 매출이 성장한 것으로 나타났다 .\n",
      "\n",
      " 특히 성장이 두드러진 브랜드는 <정관장과:ORG> <아모레퍼시픽:ORG> 등이었다 .\n",
      " 특히 성장이 두드러진 브랜드는 <정관장과:ORG> <아모레퍼시픽:ORG> 등이었다 .\n",
      "\n",
      " <에이컴메이트:ORG>는 <왕훙:PER> ( <중국:LOC> 인플루언서 ) 을 활용한 마케팅과 자체 라이브 방송을 매일 <3 ~ 4시간:DUR>씩 진행하며 <중국:LOC> 진출 기업들의 호응을 유도하고 있다 .\n",
      " <에이컴메이트:ORG>는 <왕훙:PER> ( <중국:ORG> 인플루언서 ) 을 활용한 마케팅과 자체 라이브 방송을 매일 <3:NOH> ~ <4시간:NOH>씩 진행하며 중국 진출 기업들의 호응을 유도하고 있다 .\n",
      "\n",
      " <이윤식:PER> <NHN:ORG> 커머스사업본부장 겸 <NHN고도:ORG> 대표는 \" 하반기에 <중국:LOC> 쇼핑 최대 성수기인 광군제를 포함한 다양한 행사가 진행되는 만큼 연간 거래액은 더욱 기대할 만할 것 \" 이라고 말했다 .\n",
      " <이윤식:PER> <NHN 커머스:ORG>사업본부장 겸 <NHN고도:ORG> 대표는 \" 하반기에 <중국:ORG> 쇼핑 최대 성수기인 <광군제를:DAT> 포함한 다양한 행사가 진행되는 만큼 연간 거래액은 더욱 기대할 만할 것 \" 이라고 말했다 .\n",
      "\n",
      " <경기:LOC> 변동에 민감한 구리 가격이 오름세를 보이면서 경기 회복에 대한 기대도 커지고 있다 .\n",
      " 경기 변동에 민감한 구리 가격이 오름세를 보이면서 경기 회복에 대한 기대도 커지고 있다 .\n",
      "\n",
      " 그러나 애널리스트들은 최근 가격 급등에도 이 같은 평가에 조심스러운 태도를 보였다 .\n",
      " 그러나 애널리스트들은 최근 가격 급등에도 이 같은 평가에 조심스러운 태도를 보였다 .\n",
      "\n",
      " <6일:DAT> ( 현지시간 ) <CNBC:ORG>에 따르면 <런던금속거래소 ( LME ):ORG> 에 <3개월:NOH> 만기 구리 선물 가격은 지난 <6월:DAT> 말에 톤당 <6 , 000달러를:MNY> 돌파했다 .\n",
      " <6일:DAT> ( 현지시간 ) <CNBC:ORG>에 따르면 <런던금속거래소:ORG> ( <LME:ORG> ) 에 <3개월:DUR> 만기 구리 선물 가격은 지난 <6월:DAT> 말에 톤당 <6 , 000달러:MNY>를 돌파했다 .\n",
      "\n",
      " 지난 <3월:DAT> <신종 코로나바이러스 감염증 ( 코로나19 ):POH> 우려로 기록한 저점 <4 , 626 . 50달러:MNY>에 비해 크게 오른 것이다 .\n",
      " 지난 <3월:DAT> 신종 <코로나바이러스:POH> 감염증 ( <코로나19:POH> ) 우려로 기록한 저점 <4 , 626 . 50달러:MNY>에 비해 크게 오른 것이다 .\n",
      "\n",
      " 이날 <3개월:NOH>물 구리가격은 전장보다 <1 . 2 %:PNT> 오른 <6 , 088달러:MNY>에 거래됐다 .\n",
      " 이날 <3개월:NOH>물 구리가격은 전장보다 <1 . 2 %:PNT> 오른 <6 , 088달러:MNY>에 거래됐다 .\n",
      "\n",
      " <씨티:ORG>의 애널리스트들은 보고서에서 \" 구리 가격이 지난 한 달간 톤당 <5 , 700달러:MNY>에서 <6 , 000달러:MNY>까지 오른 것은 주가와 국채 금리가 하락한 것이 영향을 미친 것 \" 이라며 \" 구리 가격과 주가 및 국채 금리와의 역대 상관관계를 고려하면 톤당 <220 ~ 4\n",
      " <씨티:ORG>의 애널리스트들은 보고서에서 \" 구리 가격이 지난 <한 달:NOH>간 톤당 <5 , 700달러:MNY>에서 <6 , 000달러:MNY>까지 오른 것은 주가와 국채 금리가 하락한 것이 영향을 미친 것 \" 이라며 \" 구리 가격과 주가 및 국채 금리와의 역대 상관관계를 고려하면 톤당 <220:MNY> ~ <4\n",
      "\n",
      " 이들은 \" 단기적으로 구리가격이 톤당 <5 , 750달러:MNY>까지 오를 것이라는 전망을 유지하며 , <2 ~ 4주:DUR>내에 조정 가능성이 엿보여 조정 때 매수에 나설 것을 추천한다 \" 고 말했다 .\n",
      " 이들은 \" 단기적으로 구리가격이 톤당 <5 , 750달러:MNY>까지 오를 것이라는 전망을 유지하며 , <2:NOH> ~ <4주:NOH>내에 조정 가능성이 엿보여 조정 때 매수에 나설 것을 추천한다 \" 고 말했다 .\n",
      "\n",
      " <삭소은행:ORG>의 애널리스트들은 이들보다 더 비관적인 전망을 내왔다 .\n",
      " <삭소은행:ORG>의 애널리스트들은 이들보다 더 비관적인 전망을 내왔다 .\n",
      "\n",
      " 이들은 경제가 몇분기 내에 정상으로 되돌아갈 것이라는 믿음이 \" 완전히 잘못될 가능성이 크다 \" 고 말했다 .\n",
      " 이들은 경제가 몇분기 내에 정상으로 되돌아갈 것이라는 믿음이 \" 완전히 잘못될 가능성이 크다 \" 고 말했다 .\n",
      "\n",
      " <삭소은행:ORG>의 <올레 한센:PER> 원자재 전략가는 \" 구리 가격이 팬데믹 이전 수준으로 회복했지만 , <3분기에:NOH> 더 높은 수준으로 가는 데 어려움이 예상된다 \" 고 말했다 .\n",
      " <삭소은행:ORG>의 <올레 한센:PER> 원자재 전략가는 \" 구리 가격이 <팬데믹:POH> 이전 수준으로 회복했지만 , <3분기에:NOH> 더 높은 수준으로 가는 데 어려움이 예상된다 \" 고 말했다 .\n",
      "\n",
      " 그는 \" 남미 광산의 공급 차질과 맞물려 중국의 수요 회복으로 인해 투기꾼들이 구리 매수 포지션을 구축하고 있다 \" 라며 \" 이로 인해 구리 가격이 파운드당 <2 . 50달러:MNY>를 넘어선 것 \" 이라고 설명했다 .\n",
      " 그는 \" <남미:LOC> 광산의 공급 차질과 맞물려 <중국의:ORG> 수요 회복으로 인해 투기꾼들이 구리 매수 포지션을 구축하고 있다 \" 라며 \" 이로 인해 구리 가격이 파운드당 <2 . 50달러:MNY>를 넘어선 것 \" 이라고 설명했다 .\n",
      "\n",
      " 그는 그러나 \" 글로벌 주요 구리 수요국인 <미국:LOC>과 <중국의:LOC> <코로나19:NOH> <2차:NOH> 파동 위험이 이러한 투자를 재고하게 할 것 \" 이라며 \" 다음 분기에 추가 상승은 없을 것 \" 이라고 말했다 .\n",
      " 그는 그러나 \" 글로벌 주요 구리 수요국인 <미국:ORG>과 <중국의:ORG> <코로나19:POH> <2차 파동:NOH> 위험이 이러한 투자를 재고하게 할 것 \" 이라며 \" 다음 분기에 추가 상승은 없을 것 \" 이라고 말했다 .\n",
      "\n",
      " <우드 매켄지:ORG>의 <엘레니 요안니데스:PER> 수석 애널리스트는 경제 봉쇄가 풀리면서 하반기에 구리 수요가 늘어날 것이라면서도 경제가 올해 내에 어느 정도 반등할지가 관건이라고 강조했다 .\n",
      " <우드 매켄지:ORG>의 <엘레니 요안니데스:PER> 수석 애널리스트는 경제 봉쇄가 풀리면서 하반기에 구리 수요가 늘어날 것이라면서도 경제가 올해 내에 어느 정도 반등할지가 관건이라고 강조했다 .\n",
      "\n",
      " 그는 사람들이 기대한 만큼 외출하지 않고 , 새 차나 새로운 가전을 사지 않는다면 구리 수요가 반등하지 않을 수 있다고 우려했다 .\n",
      " 그는 사람들이 기대한 만큼 외출하지 않고 , 새 차나 새로운 가전을 사지 않는다면 구리 수요가 반등하지 않을 수 있다고 우려했다 .\n",
      "\n",
      " <요안니데스는:PER> 앞으로 몇 년간 구리 수요는 탄탄한 증가세를 보일 것이라면서도 공급이 더 강할 수 있다며 앞으로 <2년간:DUR> 공급이 수요를 앞질러 가격이 하락 압력을 받을 수 있다고 경고했다 .\n",
      " <요안니데스는:PER> 앞으로 몇 년간 구리 수요는 탄탄한 증가세를 보일 것이라면서도 공급이 더 강할 수 있다며 앞으로 <2년간:DUR> 공급이 수요를 앞질러 가격이 하락 압력을 받을 수 있다고 경고했다 .\n",
      "\n",
      " <골드만삭스가:ORG> 올해 <미국의:LOC> <국내총생산 ( GDP ):POH> 성장률 전망치를 기존 마이너스 ( - ) <4 . 2 %:PNT> 에서 <- 4 . 6 %:PNT> 로 하향했다 .\n",
      " <골드만삭스가:ORG> 올해 <미국의:ORG> 국내총생산 ( GDP ) 성장률 전망치를 기존 마이너스 ( - ) <4 . 2 %:PNT> 에서 - <4 . 6 %:PNT> 로 하향했다 .\n",
      "\n",
      " <3분기:NOH> 성장률이 전 분기 대비 연율로 <25 %:PNT> 로 기존 전망치 <33 %:PNT> 보다 낮아질 것으로 예상됐기 때문이다 .\n",
      " <3분기:NOH> 성장률이 전 분기 대비 연율로 <25 %:PNT> 로 기존 전망치 <33 %:PNT> 보다 낮아질 것으로 예상됐기 때문이다 .\n",
      "\n",
      " <5일:DAT> ( 현지시간 ) <폭스 비즈니스:ORG> 등 외신들에 따르면 <골드만:ORG>의 이코노미스트들은 전날 발표한 보고서에서 각 주의 규제 강화와 자발적 사회적 거리 두기로 경제 활동이 눈에 띄게 제한을 받고 있다며 이번 분기 성장률 전망치를 하향한다고 말했다 .\n",
      " <5일:DAT> ( 현지시간 ) <폭스 비즈니스:ORG> 등 외신들에 따르면 <골드만:ORG>의 이코노미스트들은 전날 발표한 보고서에서 각 주의 규제 강화와 자발적 사회적 거리 두기로 경제 활동이 눈에 띄게 제한을 받고 있다며 이번 분기 성장률 전망치를 하향한다고 말했다 .\n",
      "\n",
      " 이들은 <7월:DAT>과 <8월에:DAT> 소비지출이 정체되면서 <3분기:NOH> <GDP:POH>가 연율로 전 분기 대비 <25 %:PNT> 반등하는 데 그칠 것으로 예상했다 .\n",
      " 이들은 <7월:DAT>과 <8월에:DAT> 소비지출이 정체되면서 <3분기:NOH> GDP가 연율로 전 분기 대비 <25 %:PNT> 반등하는 데 그칠 것으로 예상했다 .\n",
      "\n",
      " 이들은 다만 <미국:LOC> 경제가 <9월에는:DAT> 추세로 돌아갈 것으로 예상했다 .\n",
      " 이들은 다만 <미국:ORG> 경제가 <9월에는:DAT> 추세로 돌아갈 것으로 예상했다 .\n",
      "\n",
      " <골드만:ORG>의 이코노미스트들은 올해 분기 및 연간 성장률 전망치는 하향하면서도 내년 <GDP:POH>가 <1 ~ 1 . 5 %:PNT> 포인트가량 더 강할 것으로 예상한다며 성장률 전망치를 <5 . 8 %:PNT> 로 제시했다 . 이는 백신 전망이 개선된 것을 반영한 것이라고 설명했다 .\n",
      " <골드만:ORG>의 이코노미스트들은 올해 분기 및 연간 성장률 전망치는 하향하면서도 내년 GDP가 <1:PNT> ~ <1 . 5 % 포인트:PNT>가량 더 강할 것으로 예상한다며 성장률 전망치를 <5 . 8 %:PNT> 로 제시했다 . 이는 백신 전망이 개선된 것을 반영한 것이라고 설명했다 .\n",
      "\n",
      " 이들은 실업률은 올해 말에 <9 %:PNT> 로 하락할 것이라고 예상해 이전 전망치인 <9 . 5 %:PNT> 보다 개선될 것으로 예상했다 .\n",
      " 이들은 실업률은 올해 말에 <9 %:PNT> 로 하락할 것이라고 예상해 이전 전망치인 <9 . 5 %:PNT> 보다 개선될 것으로 예상했다 .\n",
      "\n",
      " 우리나라의 국채시장은 위기 때마다 선진제도를 도입해 한층 성장했다 . 외환위기 시절에는 국고채전문딜러를 , 글로벌 금융위기에는 국고채 교환제도와 차등 가격낙찰제도 도입 등으로 국내외 자본시장에서 자금을 끌어들여 유용한 재정자금으로 활용했다 .\n",
      " 우리나라의 국채시장은 위기 때마다 선진제도를 도입해 한층 성장했다 . 외환위기 시절에는 국고채전문딜러를 , 글로벌 금융위기에는 국고채 교환제도와 차등 가격낙찰제도 도입 등으로 국내외 자본시장에서 자금을 끌어들여 유용한 재정자금으로 활용했다 .\n",
      "\n",
      " <신종 코로나바이러스 감염증 ( 코로나19 ):POH> 등으로 국채 소요가 급증하자 , 이번에는 근본적으로 ' 국채 씽크탱크 ' 인 <국채연구센터:ORG>를 설립을 통해 대응에 나선 것이다 .\n",
      " 신종 <코로나바이러스:POH> 감염증 ( <코로나19:POH> ) 등으로 국채 소요가 급증하자 , 이번에는 근본적으로 ' 국채 씽크탱크 ' 인 <국채연구센터:ORG>를 설립을 통해 대응에 나선 것이다 .\n",
      "\n",
      " ◇ <20년새:DUR> <10배:NOH> 불어난 한국 국채시장 ... 씽크탱크로 대응\n",
      " ◇ <20년:DUR>새 <10배:PNT> 불어난 <한국:ORG> 국채시장 ... 씽크탱크로 대응\n",
      "\n",
      " <7일:DAT> 관계부처에 따르면 <기획재정부:ORG>는 내년 <1분기에:NOH> , 늦어도 상반기 안으로 <국채연구센터:ORG>를 설립할 계획이다 .\n",
      " <7일:DAT> 관계부처에 따르면 <기획재정부:ORG>는 내년 <1분기에:NOH> , 늦어도 상반기 안으로 국채연구센터를 설립할 계획이다 .\n",
      "\n",
      " <한국개발연구원 ( KDI ):ORG> 과 <한국금융연구원:ORG> , <자본시장연구원:ORG> 가운데 <1곳:NOH>을 선정해 산하에 연구조직을 꾸리는 것이다 .\n",
      " <한국개발연구원:ORG> ( <KDI:ORG> ) 과 <한국금융연구원:ORG> , <자본시장연구원:ORG> 가운데 <1곳:NOH>을 선정해 산하에 연구조직을 꾸리는 것이다 .\n",
      "\n",
      " 센터장을 포함해 <10명:NOH> 안팎의 조직으로 우리나라의 국채 발행ᆞ유통 전략을 수립하고 , 시장 경보 기능을 포함한 비상계획 ( 컨틴전시 플랜 ) 등도 마련할 방침이다 .\n",
      " 센터장을 포함해 <10명:NOH> 안팎의 조직으로 우리나라의 국채 발행ᆞ유통 전략을 수립하고 , 시장 경보 기능을 포함한 비상계획 ( 컨틴전시 플랜 ) 등도 마련할 방침이다 .\n",
      "\n",
      " <기재부:ORG> 관계자는 \" <미국:LOC>과 <일본:LOC> , <영국:LOC> 등 선진국은 국채 발행관리를 전담하는 정부 조직을 큰 규모로 보유하고 있다 \" 며 \" 우리나라의 <국채연구센터:ORG>도 비슷한 역할을 수행할 것 \" 이라고 설명했다 .\n",
      " <기재부:ORG> 관계자는 \" <미국:ORG>과 <일본:ORG> , <영국:ORG> 등 선진국은 국채 발행관리를 전담하는 정부 조직을 큰 규모로 보유하고 있다 \" 며 \" 우리나라의 국채연구센터도 비슷한 역할을 수행할 것 \" 이라고 설명했다 .\n",
      "\n",
      " <국채연구센터:ORG> 설립은 <코로나19:POH> 사태와 관련이 있다 .\n",
      " <국채연구센터:ORG> 설립은 <코로나19 사태:POH>와 관련이 있다 .\n",
      "\n",
      " <코로나19:POH>에 대응하기 위해 자본시장에 조달해야 하는 자금 수요가 늘었고 , 이는 고스란히 국채의 발행으로 이어졌다 .\n",
      " <코로나19:POH>에 대응하기 위해 자본시장에 조달해야 하는 자금 수요가 늘었고 , 이는 고스란히 국채의 발행으로 이어졌다 .\n",
      "\n",
      " 올해만 해도 <3차:NOH> 추가경정예산까지 국채 순증액은 <107조7천억원:MNY>이고 , 이 가운데 일반회계에 계상되는 적자국채는 <97조원:MNY> 수준에 달한다 . 불과 <1년:DUR> 전에 국채 순증액이 <44조5천억원:MNY> , 적자국채가 <34조3천억원:MNY>이었다는 점을 고려하면\n",
      " 올해만 해도 <3차:NOH> 추가경정예산까지 국채 순증액은 <107조7천억원:MNY>이고 , 이 가운데 일반회계에 계상되는 적자국채는 <97조원:MNY> 수준에 달한다 . 불과 <1년:NOH> 전에 국채 순증액이 <44조5천억원:MNY> , 적자국채가 <34조3천억원:MNY>이었다는 점을 고려하면\n",
      "\n",
      " 따라서 발행과 유통시장은 물론 , 나아가 선진국 사례를 참고해 우리나라 국채시장을 효율적으로 관리해야 할 필요성이 커졌다 .\n",
      " 따라서 발행과 유통시장은 물론 , 나아가 선진국 사례를 참고해 우리나라 국채시장을 효율적으로 관리해야 할 필요성이 커졌다 .\n",
      "\n",
      " 물론 , <코로나19:POH>를 따로 떼 놓더라도 국채 씽크탱크의 필요성은 그간 지속해서 제기돼 왔다 .\n",
      " 물론 , <코로나19:POH>를 따로 떼 놓더라도 국채 씽크탱크의 필요성은 그간 지속해서 제기돼 왔다 .\n",
      "\n",
      " 우리나라 국채 잔액 규모는 지난 <1999년:DAT> <66조원:MNY>에서 <2019년:DAT> <700조원:MNY>으로 <10배:NOH> 이상 증가했다 . 거래 규모도 같은 기간 <200조원:MNY>에서 <3천300조원:MNY> 수준으로 <16배:NOH> 이상 늘었다 . 지난 <2016년에는:DAT> 초장기물인 <50년:DUR>물도 찍었다 .\n",
      " 우리나라 국채 잔액 규모는 지난 <1999년:DAT> <66조원:MNY>에서 <2019년:DAT> <700조원:MNY>으로 <10배:PNT> 이상 증가했다 . 거래 규모도 같은 기간 <200조원:MNY>에서 <3천300조원:MNY> 수준으로 <16배:PNT> 이상 늘었다 . 지난 <2016년에는:DAT> 초장기물인 <50년:NOH>물도 찍었다 .\n",
      "\n",
      " 외국인 국채 투자도 작년 말 기준 <98조3천억원:MNY>으로 <16 . 1 %:PNT> 의 비중을 차지하고 있다 . 지난 <2006년:DAT> <2 . 0 %:PNT> ( <4조2천억원:MNY> ) 수준에 그쳤다는 점을 고려하면 무려 <8배:NOH> 이상 확대한 셈이다 .\n",
      " 외국인 국채 투자도 작년 말 기준 <98조3천억원:MNY>으로 <16 . 1 %:PNT> 의 비중을 차지하고 있다 . 지난 <2006년:DAT> <2 . 0 %:PNT> ( <4조2천억원:MNY> ) 수준에 그쳤다는 점을 고려하면 무려 <8배:PNT> 이상 확대한 셈이다 .\n",
      "\n",
      " ◇ 위기 때마다 국채시장 변곡점 ... 과거 사례를 보면\n",
      " ◇ 위기 때마다 국채시장 변곡점 ... 과거 사례를 보면\n",
      "\n",
      " <기재부:ORG>는 과거 외환위기와 글로벌 금융위기에 제도를 업그레이드하는 방식으로 국채시장에 효율적으로 대응했다 .\n",
      " <기재부:ORG>는 과거 외환위기와 글로벌 금융위기에 제도를 업그레이드하는 방식으로 국채시장에 효율적으로 대응했다 .\n",
      "\n",
      " 우선 외환위기 극복 과정에서 정부는 <1997년:DAT> 국고채 발행량을 <2조1천억원:MNY>에서 <1998년:DAT> <12조5천억원:MNY>으로 <6배:NOH> 가까이 늘리는데 , 공급물량 ' 폭증 ' 에 따른 시장 충격을 줄이기 위해 <1999년:DAT> 국고채 전문딜러 제도를 도입했다 .\n",
      " 우선 외환위기 극복 과정에서 정부는 <1997년:DAT> 국고채 발행량을 <2조1천억원:MNY>에서 <1998년:DAT> <12조5천억원:MNY>으로 <6배:PNT> 가까이 늘리는데 , 공급물량 ' 폭증 ' 에 따른 시장 충격을 줄이기 위해 <1999년:DAT> 국고채 전문딜러 제도를 도입했다 .\n",
      "\n",
      " 또 외국인이 우리나라의 모든 채권에 투자할 수 있도록 관련 규제도 풀었다 .\n",
      " 또 외국인이 우리나라의 모든 채권에 투자할 수 있도록 관련 규제도 풀었다 .\n",
      "\n",
      " 지난 <2000년에는:DAT> 채권시장 구조의 선진화 계획을 발표하고 국고채 통합발행제도를 도입하고 발행물량에 <10년:NOH>물을 추가했다 .\n",
      " 지난 <2000년에는:DAT> 채권시장 구조의 선진화 계획을 발표하고 국고채 통합발행제도를 도입하고 발행물량에 <10년:NOH>물을 추가했다 .\n",
      "\n",
      " <기재부:ORG>는 자금 소요가 큰 폭으로 늘어난 글로벌 금융위기 때 다시 한번 제도를 손질한다 .\n",
      " <기재부:ORG>는 자금 소요가 큰 폭으로 늘어난 글로벌 금융위기 때 다시 <한번:NOH> 제도를 손질한다 .\n",
      "\n",
      " <2009년:DAT>도 국고채 발행은 <85조원:MNY>으로 <1년:DUR> 전보다 <63 %:PNT> 급증했다 . 그간 제도를 유지하는 것은 시장에 부담을 줄 가능성이 컸고 , <기재부:ORG>는 <2009년 6월:DAT> 차등 가격낙찰방식을 도입했다 .\n",
      " <2009년도:DAT> 국고채 발행은 <85조원:MNY>으로 <1년:NOH> 전보다 <63 %:PNT> 급증했다 . 그간 제도를 유지하는 것은 시장에 부담을 줄 가능성이 컸고 , <기재부:ORG>는 <2009년 6월:DAT> 차등 가격낙찰방식을 도입했다 .\n",
      "\n",
      " 아울러 유동성을 제고하기 위해 국고채 교환제도도 시행했다 . 또 비경쟁 인수 한도와 행사금리 조정을 통해 국고채 전문딜러의 인센티브를 부여해 공급물량을 소화하게 하는 데 집중했다 .\n",
      " 아울러 유동성을 제고하기 위해 국고채 교환제도도 시행했다 . 또 비경쟁 인수 한도와 행사금리 조정을 통해 국고채 전문딜러의 인센티브를 부여해 공급물량을 소화하게 하는 데 집중했다 .\n",
      "\n",
      " <국채연구센터:ORG> 설립은 조금 더 근본적으로 시장을 분석 , 대응하자는 차원이다 .\n",
      " <국채연구센터:ORG> 설립은 조금 더 근본적으로 시장을 분석 , 대응하자는 차원이다 .\n",
      "\n",
      " 국내 증권사의 한 애널리스트는 \" <기재부:ORG>의 국채 담당 공무원은 <1 ~ 2년:DUR> 단위로 교체되기 때문에 연속성 측면에서 부족한 게 있다 \" 면서 \" 이번 연구센터는 시장 담당자들을 대거 영입해 안정적으로 목소리를 듣고 반영해야 할 것 \" 이라고 조언했다 .\n",
      " 국내 증권사의 <한:NOH> 애널리스트는 \" <기재부:ORG>의 국채 담당 공무원은 <1:NOH> ~ <2년:NOH> 단위로 교체되기 때문에 연속성 측면에서 부족한 게 있다 \" 면서 \" 이번 연구센터는 시장 담당자들을 대거 영입해 안정적으로 목소리를 듣고 반영해야 할 것 \" 이라고 조언했다 .\n",
      "\n",
      " 다른 증권사의 브로커는 \" 전담조직의 권한을 단순히 연구기능에 한정하지 않고 , 선진국처럼 실질적인 발행과 유통시장 관리 권한까지 쥐여준다면 더욱 효율적으로 운영될 것 \" 이라고 제시했다 .\n",
      " 다른 증권사의 브로커는 \" 전담조직의 권한을 단순히 연구기능에 한정하지 않고 , 선진국처럼 실질적인 발행과 유통시장 관리 권한까지 쥐여준다면 더욱 효율적으로 운영될 것 \" 이라고 제시했다 .\n",
      "\n",
      " 인터넷전문은행 <카카오뱅크:ORG>가 <7일:DAT> <오픈뱅킹:POH> 대고객 서비스를 전격 개시했다 .\n",
      " 인터넷전문은행 <카카오뱅크:ORG>가 <7일:DAT> 오픈뱅킹 대고객 서비스를 전격 개시했다 .\n",
      "\n",
      " <오픈뱅킹:POH> 서비스는 하나의 모바일 애플리케이션에서 모든 은행의 계좌 조회나 간편송금 등을 가능하도록 한 서비스로 , 지난해 <12월:DAT> 은행권 · 핀테크 사를 대상으로 전면 실시됐다 .\n",
      " 오픈뱅킹 서비스는 <하나:NOH>의 모바일 애플리케이션에서 모든 은행의 계좌 조회나 간편송금 등을 가능하도록 한 서비스로 , 지난해 <12월:DAT> 은행권 · 핀테크 사를 대상으로 전면 실시됐다 .\n",
      "\n",
      " 당시 <카카오뱅크:ORG>는 기술 개발 등을 사유로 서비스 개시 일정을 늦추고 , 참가은행으로만 타 기관에 데이터를 제공했다 .\n",
      " 당시 <카카오뱅크:ORG>는 기술 개발 등을 사유로 서비스 개시 일정을 늦추고 , 참가은행으로만 타 기관에 데이터를 제공했다 .\n",
      "\n",
      " 이번에 출시된 <카카오뱅크:ORG> <오픈뱅킹:POH> 서비스는 ' 내 계좌 ' 관리와 ' 가져오기 ' 기능에 집중했다 .\n",
      " 이번에 출시된 <카카오뱅크 오픈뱅킹:POH> 서비스는 ' 내 계좌 ' 관리와 ' 가져오기 ' 기능에 집중했다 .\n",
      "\n",
      " 우선 <카카오뱅크:ORG>에서 등록할 수 있는 타 은행 계좌는 최대 <3계좌:NOH>다 . 예 · 적금과 대출 , 휴면 계좌를 제외한 입출금계좌만 등록 가능하며 , <어카운트인포:ORG>를 통해 조회하거나 계좌번호를 직접 입력해서 등록하면 된다 .\n",
      " 우선 <카카오뱅크:POH>에서 등록할 수 있는 타 은행 계좌는 최대 <3계좌:NOH>다 . 예 · 적금과 대출 , 휴면 계좌를 제외한 입출금계좌만 등록 가능하며 , 어카운트인포를 통해 조회하거나 계좌번호를 직접 입력해서 등록하면 된다 .\n",
      "\n",
      " 특히 <어카운트인포:ORG>로 조회된 계좌를 선택할 경우 동시에 여러 계좌를 등록하는 것도 가능하다 . 등록 후에는 <카카오뱅크:ORG>에서 다른 은행 입출금 계좌 잔액을 확인할 수 있다 .\n",
      " 특히 어카운트인포로 조회된 계좌를 선택할 경우 동시에 여러 계좌를 등록하는 것도 가능하다 . 등록 후에는 <카카오뱅크:POH>에서 다른 은행 입출금 계좌 잔액을 확인할 수 있다 .\n",
      "\n",
      " 이른바 집금 기능인 ' 가져오기 ' 기능도 있다 . 다른 은행 계좌에서 <카카오뱅크:ORG> 계좌로 잔액을 가져올 수 있는 기능이다 .\n",
      " 이른바 집금 기능인 ' 가져오기 ' 기능도 있다 . 다른 은행 계좌에서 <카카오뱅크:ORG> 계좌로 잔액을 가져올 수 있는 기능이다 .\n",
      "\n",
      " ' 내 계좌로 빠른 이체하기 ' 기능에 동의를 하면 타 은행잔고를 가져올 수 있다 . 이렇게 되면 고객이 사용하는 돈을 <카카오뱅크:ORG>로 모을 수 있게 되는 만큼 카카오뱅크를 ' 주거래 계좌 ' 로 사용하는 수요도 늘어날 수 있다 .\n",
      " ' 내 계좌로 빠른 이체하기 ' 기능에 동의를 하면 타 은행잔고를 가져올 수 있다 . 이렇게 되면 고객이 사용하는 돈을 <카카오뱅크:POH>로 모을 수 있게 되는 만큼 카카오뱅크를 ' 주거래 계좌 ' 로 사용하는 수요도 늘어날 수 있다 .\n",
      "\n",
      " <카카오뱅크:ORG> 관계자는 \" <오픈뱅킹:POH> 서비스를 고객들이 더 쉽고 편리하게 쓸 수 있도록 고민하고 개발했다 \" 고 말했다 .\n",
      " <카카오뱅크:ORG> 관계자는 \" 오픈뱅킹 서비스를 고객들이 더 쉽고 편리하게 쓸 수 있도록 고민하고 개발했다 \" 고 말했다 .\n",
      "\n",
      " <카카오뱅크:ORG>는 오는 <13일부터:DAT> 다음 달 <2일까지:DAT> ' 가져오기 ' 기능을 실행하고 이벤트에 참여한 고객에게 <아이패드:POH> , <애플워치:POH> , <편의점 상품권:POH> 등을 제공하는 이벤트도 진행한다 .\n",
      " <카카오뱅크:ORG>는 오는 <13일부터:DAT> 다음 달 <2일까지:DAT> ' 가져오기 ' 기능을 실행하고 이벤트에 참여한 고객에게 <아이패드:POH> , <애플워치:POH> , 편의점 상품권 등을 제공하는 이벤트도 진행한다 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in valid_dataloader2:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_mask_id, b_type_id, b_labels_id = batch\n",
    "\n",
    "    outputs = model(b_input_ids, b_mask_id, b_type_id)\n",
    "    output_tags = model.crf.decode(outputs)\n",
    "        \n",
    "    n_batch = outputs.size(0)\n",
    "    for ii in range(n_batch):\n",
    "        in_ids = data_test2['input_ids'][ii].numpy()\n",
    "        tag_ids = data_test2['label_ids'][ii].numpy()\n",
    "        tag_ids2 = output_tags[ii]\n",
    "        #print(tag_ids)\n",
    "        #print(tag_ids2)\n",
    "        ret = decoder(in_ids, tag_ids)\n",
    "        ret2 = decoder(in_ids, tag_ids2)\n",
    "        print(ret[-1])\n",
    "        print(ret2[-1])\n",
    "        #print('------------------')\n",
    "        print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,  688, 1019, 7407, 3745, 6738,  517, 6385, 7086,  605, 7126,\n",
       "        714, 3318,  838, 7207, 7119, 2945, 6556, 1326, 7276, 5448, 3318,\n",
       "       6969, 6410, 1832,  993, 7941, 7350,  522,  517,  295,  517,   40,\n",
       "       2272, 7088, 2986, 7836, 3413, 7108, 4257, 5436, 5103, 7088, 4197,\n",
       "       7207,  611,  517,  463,  617, 5357,  517, 7934,  627, 5357, 5103,\n",
       "       6079, 1550, 6135,  905,  517,  502,  517, 7102, 2261,  517,   54,\n",
       "          3,    1,    1,    1,    1,    1,    1,    1,    1])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input', 'label', 'input_ids', 'attention_mask', 'type_id', 'label_ids'])\n",
      "tensor([   2,  688, 1019, 7407, 3745, 6738,  517, 6385, 7086,  605, 7126,  714,\n",
      "        3318,  838, 7207, 7119, 2945, 6556, 1326, 7276, 5448, 3318, 6969, 6410,\n",
      "        1832,  993, 7941, 7350,  522,  517,  295,  517,   40, 2272, 7088, 2986,\n",
      "        7836, 3413, 7108, 4257, 5436, 5103, 7088, 4197, 7207,  611,  517,  463,\n",
      "         617, 5357,  517, 7934,  627, 5357, 5103, 6079, 1550, 6135,  905,  517,\n",
      "         502,  517, 7102, 2261,  517,   54,    3,    1,    1,    1,    1,    1,\n",
      "           1,    1,    1])\n",
      "tensor([22,  5, 20, 20,  6, 16, 20, 20, 20,  0, 10, 20, 20, 20, 20, 20,  8, 18,\n",
      "        18, 18, 18, 18, 18, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20, 20, 20, 20,  4, 20, 20,  4, 14, 20, 20,  4, 14,\n",
      "        20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 23, 21, 21, 21, 21, 21,\n",
      "        21, 21, 21])\n"
     ]
    }
   ],
   "source": [
    "ii = 0\n",
    "print(data_test.keys())\n",
    "print(data_test['input_ids'][ii])\n",
    "print(data_test['label_ids'][ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_ix(arr):\n",
    "    arr = list(filter(lambda x: x[-1] in 'BI', arr))\n",
    "    if len(arr) < 1:\n",
    "        return 0\n",
    "    elm_dic = {}\n",
    "    for i, x in enumerate(arr):\n",
    "        elm_dic.setdefault(i, 0)\n",
    "        elm_dic[i] += 1\n",
    "    counts = list(elm_dic.values())\n",
    "    keys = list(elm_dic.keys())\n",
    "    max_ix = counts.index(max(counts))\n",
    "    return keys[max_ix]\n",
    "    \n",
    "def find_span_location(raw_sent, n_max):\n",
    "    span_dic = {}\n",
    "    count = 0\n",
    "    for i, word in enumerate(raw_sent):\n",
    "        tokened_word = vectorizer.tokenizer(word)\n",
    "        n_subword = len(tokened_word)\n",
    "        span_dic[i] = list(range(count, count+n_subword))\n",
    "        count += n_subword\n",
    "        if count >= n_max: break\n",
    "    return span_dic\n",
    "\n",
    "def post_process(raw_sent, label_o):\n",
    "    n_max = len(label_o)\n",
    "    span_dic = find_span_location(raw_sent, n_max)\n",
    "    label_o2 = [None]*len(span_dic)\n",
    "    \n",
    "    for ix, ixs in span_dic.items():\n",
    "        label_o2[ix] = []\n",
    "        for i in ixs:\n",
    "            if i >= n_max:\n",
    "                break\n",
    "            #print('i', i, 'ix', ix)\n",
    "            label_o2[ix].append(label_o[i])\n",
    "    final_label = []\n",
    "    for wordset in label_o2:\n",
    "        max_ix = get_max_ix(wordset)\n",
    "        final_label.append(wordset[max_ix])\n",
    "                \n",
    "    return final_label, label_o2\n",
    "\n",
    "ii = 34\n",
    "sent1 = sentences_test[ii]\n",
    "sent2 = data_test['input'][ii]\n",
    "label_o = data_test['label'][ii]\n",
    "\n",
    "label_dic = find_span_location(sent1, 75)\n",
    "print(sent1)\n",
    "print(label_dic)\n",
    "print()\n",
    "print(sent2)\n",
    "print(label_o)\n",
    "print()\n",
    "\n",
    "final_label, label2 = post_process(sent1, label_o)\n",
    "print(label2)\n",
    "print(final_label)\n",
    "print()\n",
    "print(data_train['label_ids'][ii].detach().to('cpu').numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "b_input_ids : padded and tokened\n",
    "b_labels_id : padded and tokened\n",
    "pred_label : padded and tokened\n",
    "label_o : tokened <-- need post processing\n",
    "\n",
    "\"\"\"\n",
    "def size_limit(seq):\n",
    "    if len(seq) >= MAX_LEN-2:\n",
    "        return seq[:MAX_LEN-2]\n",
    "    else:\n",
    "        return seq\n",
    "\n",
    "test_dataloader = DataLoader(dataset_test, sampler=valid_sampler, batch_size=1)\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    #if i>300: break\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_mask_id, b_type_id, b_labels_id = batch\n",
    "\n",
    "    label_o = data_test['label'][i]\n",
    "    label_o = size_limit(label_o)\n",
    "    \n",
    "    sent = sentences_test[i]\n",
    "    final_label, _ = post_process(sent, label_o)\n",
    "    all_labels += final_label\n",
    "    \n",
    "    \n",
    "    #print('i', i)\n",
    "    \n",
    "    #print('label', final_label)\n",
    "    len_label = min([len(label_o), MAX_LEN-2])\n",
    "    pred_id = model(b_input_ids, b_mask_id, b_type_id)\n",
    "    #pred = pred_id.detach().cpu().numpy()[0, 1:1+len_label]\n",
    "    #print(i,len_label, len(label_o), len(pred_label), len(sent))\n",
    "    #pred = np.argmax(pred, axis=-1)\n",
    "    pred_id = model.crf.decode(pred_id)\n",
    "    pred = pred_id[0][1:1+len_label]\n",
    "    pred_tag = [ idx2tag[idx] for idx in pred ]\n",
    "    #print('pred', pred_label_tag)\n",
    "    #print(pred_label_tag.shape)\n",
    "\n",
    "    final_pred_tag, _ = post_process(sent, pred_tag)\n",
    "    all_preds += final_pred_tag\n",
    "    \n",
    "    if len(final_label) != len(final_pred_tag):\n",
    "        print('i: ', i, )\n",
    "        print(i,len_label, len(label_o), len(pred_tag), len(sent))\n",
    "        print('final_label', len(final_label), 'final_pred', len(final_pred_tag))\n",
    "\n",
    "print('f1_score', \n",
    "      f1_score(all_labels, all_preds, average='macro', labels=eff_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'label', 'input_ids', 'attention_mask', 'type_id', 'label_ids'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"\"\"\n",
    "Mr. Trump’s tweets began just moments after a Fox News report by Mike Tobin, a \n",
    "reporter for the network, about protests in Minnesota and elsewhere. \n",
    "\"\"\"\n",
    "test_sentence = \"Hugging Face Inc. is a company based in New York City. Its headquarters are in DUMBO, therefore very\" \\\n",
    "           \"close to the Manhattan Bridge which is visible from the window.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "input_ids = torch.tensor([tokenized_sentence]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join bpe split tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "new_tokens, new_labels = [], []\n",
    "for token, label_idx in zip(tokens, label_indices[0]):\n",
    "    if token.startswith(\"##\"):\n",
    "        new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "    else:\n",
    "        new_labels.append(tag_values[label_idx])\n",
    "        new_tokens.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O\t[CLS]\n",
      "B-org\thugging\n",
      "I-org\tface\n",
      "I-org\tinc\n",
      "I-org\t.\n",
      "O\tis\n",
      "O\ta\n",
      "O\tcompany\n",
      "O\tbased\n",
      "O\tin\n",
      "B-geo\tnew\n",
      "I-geo\tyork\n",
      "I-geo\tcity\n",
      "O\t.\n",
      "O\tits\n",
      "O\theadquarters\n",
      "O\tare\n",
      "O\tin\n",
      "B-geo\tdumbo\n",
      "O\t,\n",
      "O\ttherefore\n",
      "O\tveryclose\n",
      "O\tto\n",
      "O\tthe\n",
      "B-geo\tmanhattan\n",
      "I-geo\tbridge\n",
      "O\twhich\n",
      "O\tis\n",
      "O\tvisible\n",
      "O\tfrom\n",
      "O\tthe\n",
      "O\twindow\n",
      "O\t.\n",
      "O\t[SEP]\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(new_tokens, new_labels):\n",
    "    print(\"{}\\t{}\".format(label, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

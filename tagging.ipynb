{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging\n",
    "Each dataset has different types of tagging.  \n",
    "This is applied to NER tagging.  \n",
    "Each dataset has its own tagging and each model uses different tokenizer.  \n",
    "To calculate accuracies, we need to arrange tags to compare properly.\n",
    "In this notebook, two different tokens of model and tagging are adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXO-brain form\n",
    "\"\"\"\n",
    "## 1\n",
    "## 오에 겐자부로는 일본 현대문학의 초석을 \n",
    "## <오에 겐자부로:PER>는 <일본:LOC> 현대문학의 초석을 \n",
    "오에\t오에\tNNG\tB-PER\n",
    "_\t_\t_\tI-PER\n",
    "겐자부로\t겐자부로\tNNP\tI-PER\n",
    "는\t는\tJX\tO\n",
    "\n",
    "## 2\n",
    "## 이미 수상자(2000년 김대중 전 대통령)를 배출한 데다\n",
    "## 이미 수상자(<2000년:DAT> <김대중:PER> 전 대통령)를 배출한 데다\n",
    "이미\t이미\tMAG\tO\n",
    "_\t_\t_\tO\n",
    "수상자\t수상자\tNNG\tO\n",
    "(\t(\tSS\tO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"오에\t오에\tNNG\tB-PER\n",
    "_\t_\t_\tI-PER\n",
    "겐자부로\t겐자부로\tNNP\tI-PER\n",
    "는\t는\tJX\tO\n",
    "_\t_\t_\tO\n",
    "일본\t일본\tNNP\tB-LOC\n",
    "_\t_\t_\tO\n",
    "현대\t현대\tNNG\tO\n",
    "문학\t문학\tNNG\tO\n",
    "의\t의\tJKG\tO\n",
    "_\t_\t_\tO\n",
    "초석\t초석\tNNG\tO\n",
    "을\t을\tJKO\tO\n",
    "_\t_\t_\tO\n",
    "놓\t놓\tVV\tO\n",
    "은\t은\tETM\tO\n",
    "_\t_\t_\tO\n",
    "것\t것\tNNB\tO\n",
    "으로\t으로\tJKB\tO\n",
    "_\t_\t_\tO\n",
    "평가\t평가\tNNG\tO\n",
    "받\t받\tVV\tO\n",
    "는\t는\tETM\tO\n",
    "_\t_\t_\tO\n",
    "작가\t작가\tNNG\tO\n",
    "_\t_\t_\tO\n",
    "나쓰메\t나쓰메\tNNP\tB-PER\n",
    "_\t_\t_\tI-PER\n",
    "소세키\t소세키\tNNP\tI-PER\n",
    "(\t(\tSS\tO\n",
    "1867\t1867\tSN\tB-DUR\n",
    "~\t~\tSO\tI-DUR\n",
    "1916\t1916\tSN\tI-DUR\n",
    ")\t)\tSS\tO\n",
    "의\t의\tJKG\tO\n",
    "_\t_\t_\tO\n",
    "대표작\t대표작\tNNG\tO\n",
    "_\t_\t_\tO\n",
    "‘\t‘\tSS\tO\n",
    "마음\t마음\tNNG\tB-POH\n",
    "’\t’\tSS\tO\n",
    "에\t에\tJKB\tO\n",
    "_\t_\t_\tO\n",
    "담긴\t담기+ㄴ\tVV+ETM\tO\n",
    "_\t_\t_\tO\n",
    "군국주의\t군국주의\tNNG\tO\n",
    "적\t적\tXSN\tO\n",
    "_\t_\t_\tO\n",
    "요소\t요소\tNNG\tO\n",
    ",\t,\tSP\tO\n",
    "_\t_\t_\tO\n",
    "야스쿠니\t야스쿠니\tNNP\tB-ORG\n",
    "_\t_\t_\tI-ORG\n",
    "신사\t신사\tNNG\tI-ORG\n",
    "_\t_\t_\tO\n",
    "참배\t참배\tNNG\tO\n",
    "_\t_\t_\tO\n",
    "행위\t행위\tNNG\tO\n",
    "까지\t까지\tJX\tO\n",
    "_\t_\t_\tO\n",
    "소설\t소설\tNNG\tO\n",
    "의\t의\tJKG\tO\n",
    "_\t_\t_\tO\n",
    "삽화\t삽화\tNNG\tO\n",
    "로\t로\tJKB\tO\n",
    "_\t_\t_\tO\n",
    "동원하\t동원하\tVV\tO\n",
    "며\t며\tEC\tO\n",
    "_\t_\t_\tO\n",
    "일본\t일본\tNNP\tB-ORG\n",
    "_\t_\t_\tO\n",
    "사회\t사회\tNNG\tO\n",
    "의\t의\tJKG\tO\n",
    "_\t_\t_\tO\n",
    "‘\t‘\tSS\tO\n",
    "비정상\t비정상\tNNG\tO\n",
    "성\t성\tXSN\tO\n",
    "’\t’\tSS\tO\n",
    "을\t을\tJKO\tO\n",
    "_\t_\t_\tO\n",
    "문제\t문제\tNNG\tO\n",
    "_\t_\t_\tO\n",
    "삼\t삼\tVV\tO\n",
    "는다\t는다\tEF\tO\n",
    ".\t.\tSF\tO\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [ line1.split('\\t')[0] for line1 in test.split('\\n') if line1.split('\\t')[2] != '_' ]\n",
    "#print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKT KoBERT\n",
    "import gluonnlp as nlp\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from transformers import BertTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "# KoBERT tokenizer\n",
    "kobert, vocab = get_pytorch_kobert_model()\n",
    "tokenizer = nlp.data.BERTSPTokenizer(get_tokenizer(), vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁오', '에', '▁', '겐', '자', '부', '로', '는', '▁일본', '▁현대', '문학', '의', '▁초', '석', '을', '▁놓', '은', '▁것으로', '▁평가', '받는', '▁작가', '▁나', '쓰', '메', '▁소', '세', '키', '▁(', '▁18', '67', '▁', '~', '▁19', '16', '▁', ')', '▁', '의', '▁대표', '작', '▁‘', '▁마음', '▁', '’', '▁', '에', '▁담긴', '▁군', '국', '주의', '적', '▁요소', '▁', ',', '▁야', '스', '쿠', '니', '▁신', '사', '▁참', '배', '▁행위', '까지', '▁소설', '의', '▁', '삽', '화', '로', '▁동원', '하며', '▁일본', '▁사회', '의', '▁‘', '▁비', '정', '상', '성', '▁', '’', '▁', '을', '▁문제', '▁삼', '는', '다', '▁', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"오에 겐자부로는 일본 현대문학의 초석을 놓은 것으로 평가받는 작가 나쓰메 소세키(1867~1916)의 대표작 ‘마음’에 담긴 군국주의적 요소, 야스쿠니 신사 참배 행위까지 소설의 삽화로 동원하며 일본 사회의 ‘비정상성’을 문제 삼는다.\"\n",
    "tag_text = \"<오에 겐자부로:PER>는 <일본:LOC> 현대문학의 초석을 놓은 것으로 평가받는 작가 <나쓰메 소세키:PER>(<1867~1916:DUR>)의 대표작 ‘<마음:POH>’에 담긴 군국주의적 요소, <야스쿠니 신사:ORG> 참배 행위까지 소설의 삽화로 동원하며 <일본:ORG> 사회의 ‘비정상성’을 문제 삼는다.\"\n",
    "tokens = tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오에', '겐자부로', '는', '일본', '현대', '문학', '의', '초석', '을', '놓', '은', '것', '으로', '평가', '받', '는', '작가', '나쓰메', '소세키', '(', '1867', '~', '1916', ')', '의', '대표작', '‘', '마음', '’', '에', '담긴', '군국주의', '적', '요소', ',', '야스쿠니', '신사', '참배', '행위', '까지', '소설', '의', '삽화', '로', '동원하', '며', '일본', '사회', '의', '‘', '비정상', '성', '’', '을', '문제', '삼', '는다', '.']\n",
      "\n",
      "['▁오', '에', '▁', '겐', '자', '부', '로', '▁', '는', '▁일본', '▁현대', '▁', '문학', '▁', '의', '▁초', '석', '▁', '을', '▁놓', '▁', '은', '▁것', '▁', '으로', '▁평가', '▁받', '▁', '는', '▁작가', '▁나', '쓰', '메', '▁소', '세', '키', '▁(', '▁18', '67', '▁', '~', '▁19', '16', '▁', ')', '▁', '의', '▁대표', '작', '▁‘', '▁마음', '▁', '’', '▁', '에', '▁담긴', '▁군', '국', '주의', '▁적', '▁요소', '▁', ',', '▁야', '스', '쿠', '니', '▁신', '사', '▁참', '배', '▁행위', '▁', '까지', '▁소설', '▁', '의', '▁', '삽', '화', '▁', '로', '▁동원', '하', '▁', '며', '▁일본', '▁사회', '▁', '의', '▁‘', '▁비', '정', '상', '▁성', '▁', '’', '▁', '을', '▁문제', '▁삼', '▁', '는', '다', '▁', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = []\n",
    "for text1 in texts:\n",
    "    ret.extend(tokenizer(text1))\n",
    "print(texts)\n",
    "print()\n",
    "print(ret)\n",
    "ret == tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEP]\n",
      "(18\n",
      "67\n",
      "▁(\n",
      "[PAD]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab.idx_to_token[3])\n",
    "print(tokenizer.vocab.idx_to_token[25])\n",
    "print(tokenizer.vocab.idx_to_token[199])\n",
    "print(tokenizer.vocab.idx_to_token[522])\n",
    "print(tokenizer.vocab.idx_to_token[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517\n",
      "517\n",
      "365\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab.token_to_idx['▁'])\n",
    "print(tokenizer.vocab.token_to_idx['▁'])\n",
    "print(tokenizer.vocab.token_to_idx['_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁오\n",
      "에\n",
      "▁\n",
      "겐\n",
      "자\n",
      "부\n",
      "로\n",
      "는\n",
      "▁일본\n",
      "▁현대\n",
      "문학\n",
      "의\n",
      "▁초\n"
     ]
    }
   ],
   "source": [
    "arr = '3417 6896  517 5402 7147 6398 6079 5760 3809 5051 6237 7095 4501'.split()\n",
    "for x in arr:\n",
    "    print(tokenizer.vocab.idx_to_token[int(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenizer\n",
    "B_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_bert.BertTokenizer"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(B_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('카카오뱅크는', (0, 6)), ('오는', (7, 9)), ('13일부터', (10, 15)), ('다음', (16, 18)), ('달', (19, 20)), ('2일까지', (21, 25)), (\"'가져오기'\", (26, 32)), ('기능을', (33, 36)), ('실행하고', (37, 41)), ('이벤트에', (42, 46)), ('참여한', (47, 50)), ('고객에게', (51, 55)), ('아이패드,', (56, 61)), ('애플워치,', (62, 67)), ('편의점', (68, 71)), ('상품권', (72, 75)), ('등을', (76, 78)), ('제공하는', (79, 83)), ('이벤트도', (84, 88)), ('진행한다.', (89, 94))]\n",
      "\n",
      "[('▁카카오', (0, 3)), ('뱅', (3, 4)), ('크', (4, 5)), ('는', (5, 6)), ('▁오는', (7, 9)), ('▁13', (10, 12)), ('일부터', (12, 15)), ('▁다음', (16, 18)), ('▁달', (19, 20)), ('▁2', (21, 22)), ('일까지', (22, 25)), (\"▁'\", (26, 27)), ('▁가져', (27, 29)), ('오', (29, 30)), ('기', (30, 31)), (\"▁'\", (31, 32)), ('▁기능을', (33, 36)), ('▁실행', (37, 39)), ('하고', (39, 41)), ('▁이벤트', (42, 45)), ('에', (45, 46)), ('▁참여한', (47, 50)), ('▁고객에게', (51, 55)), ('▁아이', (56, 58)), ('패', (58, 59)), ('드', (59, 60)), ('▁', (60, 60)), (',', (60, 61)), ('▁애플', (62, 64)), ('워', (64, 65)), ('치', (65, 66)), ('▁', (66, 66)), (',', (66, 67)), ('▁편의점', (68, 71)), ('▁상품', (72, 74)), ('권', (74, 75)), ('▁등을', (76, 78)), ('▁제공하는', (79, 83)), ('▁이벤트', (84, 87)), ('도', (87, 88)), ('▁진행한다', (89, 93)), ('▁', (93, 93)), ('.', (93, 94))]\n",
      "\n",
      "[('카', (0, 1)), ('##카', (1, 1)), ('##오', (1, 1)), ('##뱅', (1, 1)), ('##크', (1, 1)), ('##는', (1, 1)), ('오', (2, 3)), ('##는', (3, 3)), ('13일', (10, 13)), ('##부터', (13, 13)), ('다음', (16, 18)), ('달', (19, 20)), ('2일', (21, 23)), ('##까지', (23, 23)), (\"'\", (26, 27)), ('가', (27, 28)), ('##져', (28, 28)), ('##오', (28, 28)), ('##기', (28, 28)), (\"'\", (31, 32)), ('기', (33, 34)), ('##능을', (34, 34)), ('실', (37, 38)), ('##행', (38, 38)), ('##하고', (38, 38)), ('이', (42, 43)), ('##벤', (43, 43)), ('##트', (43, 43)), ('##에', (43, 43)), ('참', (47, 48)), ('##여', (48, 48)), ('##한', (48, 48)), ('고', (51, 52)), ('##객', (52, 52)), ('##에게', (52, 52)), ('아', (56, 57)), ('##이', (57, 57)), ('##패', (57, 57)), ('##드', (57, 57)), (',', (60, 61)), ('애', (62, 63)), ('##플', (63, 63)), ('##워', (63, 63)), ('##치', (63, 63)), (',', (66, 67)), ('편', (68, 69)), ('##의', (69, 69)), ('##점', (69, 69)), ('상', (72, 73)), ('##품', (73, 73)), ('##권', (73, 73)), ('등을', (76, 78)), ('제', (79, 80)), ('##공', (80, 80)), ('##하는', (80, 80)), ('이', (84, 85)), ('##벤', (85, 85)), ('##트', (85, 85)), ('##도', (85, 85)), ('진', (89, 90)), ('##행', (90, 90)), ('##한다', (90, 90)), ('.', (93, 94))]\n"
     ]
    }
   ],
   "source": [
    "def split_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def token_loc(text, tokenizer):\n",
    "    \"\"\"\n",
    "    For a given tokenizer, obtain each token and its position in the original text\n",
    "    \"\"\"\n",
    "    if isinstance(tokenizer, BertTokenizer):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "    else:\n",
    "        tokens = tokenizer(text)\n",
    "    token_word_loc = []\n",
    "    i_pos = 0\n",
    "    for token in tokens:\n",
    "        tokenp = token\n",
    "        if '▁' in token: # padding for word2piece\n",
    "            if len(token) > 1:\n",
    "                tokenp = token[1:]\n",
    "            else:\n",
    "                tokenp = ' '\n",
    "        i_pos, f_pos = find_loc(text, tokenp, start_i=i_pos)\n",
    "        temp = (token, (i_pos, f_pos))\n",
    "        token_word_loc.append(temp)\n",
    "        i_pos = f_pos\n",
    "    return token_word_loc\n",
    "# test\n",
    "split_tokens = token_loc(text, split_tokenizer)\n",
    "print(split_tokens)\n",
    "print()\n",
    "wp_tokens = token_loc(text, tokenizer)\n",
    "print(wp_tokens)\n",
    "print()\n",
    "B_tokens = token_loc(text, B_tokenizer)\n",
    "print(B_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_text <카카오뱅크:ORG>는 오는 <13일:DAT>부터 다음 달 <2일:DAT>까지 '가져오기' 기능을 실행하고 이벤트에 참여한 고객에게 <아이패드:POH>, <애플워치:POH>, <편의점:POH>< 상품권:POH> 등을 제공하는 이벤트도 진행한다.\n",
      "\n",
      "text 카카오뱅크는 오는 13일부터 다음 달 2일까지 '가져오기' 기능을 실행하고 이벤트에 참여한 고객에게 아이패드, 애플워치, 편의점 상품권 등을 제공하는 이벤트도 진행한다.\n",
      "\n",
      "ne_+exp <(.+?):([A-Z]{3})>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ner_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-64e968517674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ne_+exp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mne_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mner_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_NE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mne_exp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mner_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ner_ids' is not defined"
     ]
    }
   ],
   "source": [
    "def find_loc(text, subword, start_i=0):\n",
    "    \"\"\"\n",
    "    In a text, gives index of subword from start_i\n",
    "    in case subword is space, prohibit it matches with spaces \n",
    "    behind of sentences\n",
    "    :return: \n",
    "    i_loc, f_loc\n",
    "    \"\"\"\n",
    "    if subword == ' ':\n",
    "        if text[start_i] == ' ':\n",
    "            return start_i, start_i+1\n",
    "        else:\n",
    "            return start_i, start_i\n",
    "    try:\n",
    "        i_loc = start_i + text[start_i:].index(subword)\n",
    "        f_loc = i_loc + len(subword)\n",
    "    except:\n",
    "        i_loc = start_i\n",
    "        f_loc = i_loc\n",
    "    \n",
    "    return i_loc, f_loc\n",
    "# test\n",
    "#find_loc('I love youy love', 'love', start_i = 5)\n",
    "\n",
    "import re\n",
    "def get_NE(tag_text, text, ne_exp='<(.+?):([A-Z]{3})>'):\n",
    "    \"\"\"\n",
    "    find Named Entity from given text\n",
    "    :return: \n",
    "    [[NE, TAG, (i_pos, f_pos)]]\n",
    "    \"\"\"\n",
    "    ne_word_loc = []\n",
    "    reg_exp = re.compile(ne_exp)\n",
    "    nes = reg_exp.finditer(tag_text)\n",
    "    i_pos = 0\n",
    "    for ne in nes:\n",
    "        subword, tag = ne[1], ne[2]\n",
    "        i_pos, f_pos = find_loc(text, subword, start_i=i_pos)\n",
    "        temp = [subword, tag, (i_pos, f_pos)]\n",
    "        ne_word_loc.append(temp)\n",
    "    return ne_word_loc\n",
    "    \n",
    "ne_exp = '<(.+?):([A-Z]{3})>'\n",
    "\n",
    "text = \"카카오뱅크는 오는 13일부터 다음 달 2일까지 '가져오기' 기능을 실행하고 이벤트에 참여한 고객에게 아이패드, 애플워치, 편의점 상품권 등을 제공하는 이벤트도 진행한다.\\n\"\n",
    "tag_text = \"<카카오뱅크:ORG>는 오는 <13일:DAT>부터 다음 달 <2일:DAT>까지 '가져오기' 기능을 실행하고 이벤트에 참여한 고객에게 <아이패드:POH>, <애플워치:POH>, <편의점 상품권:POH> 등을 제공하는 이벤트도 진행한다.\\n\"\n",
    "\n",
    "print('tag_text', tag_text)\n",
    "print('text', text)\n",
    "print('ne_+exp', ne_exp)\n",
    "ner_tokens = get_NE(tag_text, text, ne_exp)\n",
    "print(ner_ids)\n",
    "print(ner_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(tag_tokens, model_tokens):\n",
    "    \"\"\"\n",
    "    :in: \n",
    "    tokens1, tokens2 are in form of [(word, pos)], pos = (i_loc, f_loc)\n",
    "    f_loc is inclusive\n",
    "    :desc:\n",
    "    if some forms are not adequate like empty subword, it can be removed by post-processing\n",
    "    \"\"\"\n",
    "    matching_ids = []\n",
    "    for tgt_token in tag_tokens:\n",
    "        tgt_i_loc, tgt_f_loc = tgt_token[-1]\n",
    "        i_id, f_id = 0, 0\n",
    "        for i_token, token in enumerate(model_tokens):\n",
    "            i_loc, f_loc = token[-1]\n",
    "            # case1\n",
    "            if tgt_i_loc >= i_loc and tgt_i_loc < f_loc:\n",
    "                i_id = i_token\n",
    "            # case2\n",
    "            if tgt_f_loc > i_loc and tgt_i_loc <= f_loc:\n",
    "                f_id = i_token\n",
    "        matching_ids.append((i_id, f_id))\n",
    "    return matching_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main (preprocessing)\n",
    "get tags from matching ids of (tagging ner_tokens, model wp_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['카카오뱅크', 'ORG', (0, 5)],\n",
       " ['13일', 'DAT', (10, 13)],\n",
       " ['2일', 'DAT', (21, 23)],\n",
       " ['아이패드', 'POH', (56, 60)],\n",
       " ['애플워치', 'POH', (62, 66)],\n",
       " ['편의점', 'POH', (68, 71)],\n",
       " [' 상품권', 'POH', (71, 75)]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('▁카카오', (0, 3)),\n",
       " ('뱅', (3, 4)),\n",
       " ('크', (4, 5)),\n",
       " ('는', (5, 6)),\n",
       " ('▁오는', (7, 9)),\n",
       " ('▁13', (10, 12)),\n",
       " ('일부터', (12, 15)),\n",
       " ('▁다음', (16, 18)),\n",
       " ('▁달', (19, 20)),\n",
       " ('▁2', (21, 22)),\n",
       " ('일까지', (22, 25)),\n",
       " (\"▁'\", (26, 27)),\n",
       " ('▁가져', (27, 29)),\n",
       " ('오', (29, 30)),\n",
       " ('기', (30, 31)),\n",
       " (\"▁'\", (31, 32)),\n",
       " ('▁기능을', (33, 36)),\n",
       " ('▁실행', (37, 39)),\n",
       " ('하고', (39, 41)),\n",
       " ('▁이벤트', (42, 45)),\n",
       " ('에', (45, 46)),\n",
       " ('▁참여한', (47, 50)),\n",
       " ('▁고객에게', (51, 55)),\n",
       " ('▁아이', (56, 58)),\n",
       " ('패', (58, 59)),\n",
       " ('드', (59, 60)),\n",
       " ('▁', (60, 60)),\n",
       " (',', (60, 61)),\n",
       " ('▁애플', (62, 64)),\n",
       " ('워', (64, 65)),\n",
       " ('치', (65, 66)),\n",
       " ('▁', (66, 66)),\n",
       " (',', (66, 67)),\n",
       " ('▁편의점', (68, 71)),\n",
       " ('▁상품', (72, 74)),\n",
       " ('권', (74, 75)),\n",
       " ('▁등을', (76, 78)),\n",
       " ('▁제공하는', (79, 83)),\n",
       " ('▁이벤트', (84, 87)),\n",
       " ('도', (87, 88)),\n",
       " ('▁진행한다', (89, 93)),\n",
       " ('▁', (93, 93)),\n",
       " ('.', (93, 94))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B-POH ▁카카오\n",
      "I-POH 뱅\n",
      "I-POH 크\n",
      "I-POH 는\n",
      "I-POH ▁오는\n",
      "I-POH ▁13\n",
      "I-POH 일부터\n",
      "I-POH ▁다음\n",
      "I-POH ▁달\n",
      "I-POH ▁2\n",
      "I-POH 일까지\n",
      "I-POH ▁'\n",
      "I-POH ▁가져\n",
      "I-POH 오\n",
      "I-POH 기\n",
      "I-POH ▁'\n",
      "I-POH ▁기능을\n",
      "I-POH ▁실행\n",
      "I-POH 하고\n",
      "I-POH ▁이벤트\n",
      "I-POH 에\n",
      "I-POH ▁참여한\n",
      "I-POH ▁고객에게\n",
      "I-POH ▁아이\n",
      "I-POH 패\n",
      "I-POH 드\n",
      "I-POH ▁\n",
      "I-POH ,\n",
      "I-POH ▁애플\n",
      "I-POH 워\n",
      "I-POH 치\n",
      "I-POH ▁\n",
      "I-POH ,\n",
      "I-POH ▁편의점\n",
      "I-POH ▁상품\n",
      "I-POH 권\n",
      "None ▁등을\n",
      "None ▁제공하는\n",
      "None ▁이벤트\n",
      "None 도\n",
      "None ▁진행한다\n",
      "None ▁\n",
      "None .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' test\\nmatching_ids = match(split_tokens, wp_tokens)\\nii = 0\\nfor i_id, f_id in matching_ids:\\n    #print(i_id, f_id)\\n    print(split_tokens[ii][0], wp_tokens[i_id][0], wp_tokens[f_id][0])\\n    ii += 1\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get matching ids\n",
    "matching_ids = match(ner_tokens, wp_tokens)\n",
    "\n",
    "# preprocess (TAG --> B-TAG or I-TAG)\n",
    "tg_tags = [(i, x[1]) for i, x in enumerate(ner_tokens)]\n",
    "tags = [None]*len(wp_tokens)\n",
    "for ii, tag in tg_tags:\n",
    "    i_id, f_id = matching_ids[ii]\n",
    "    tags[i_id] = 'B-'+tag\n",
    "    for jj in range(i_id+1, f_id+1):\n",
    "        tags[jj] = 'I-'+tag\n",
    "\n",
    "# test\n",
    "for ii, tag in enumerate(wp_tokens):\n",
    "    print(tags[ii], tag[0])\n",
    "    \n",
    "\"\"\" test\n",
    "matching_ids = match(split_tokens, wp_tokens)\n",
    "ii = 0\n",
    "for i_id, f_id in matching_ids:\n",
    "    #print(i_id, f_id)\n",
    "    print(split_tokens[ii][0], wp_tokens[i_id][0], wp_tokens[f_id][0])\n",
    "    ii += 1\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'PER'),\n",
       " (1, 'LOC'),\n",
       " (2, 'PER'),\n",
       " (3, 'DUR'),\n",
       " (4, 'POH'),\n",
       " (5, 'ORG'),\n",
       " (6, 'ORG')]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 오 1 에 2   3 겐 4 자 5 부 6 로 7 는 8   9 일 10 본 11   12 현 13 대 14 문 15 학 16 의 17   18 초 19 석 20 을 21   22 놓 23 은 24   25 것 26 으 27 로 28   29 평 30 가 31 받 32 는 33   34 작 35 가 36   37 나 38 쓰 39 메 40   41 소 42 세 43 키 44 ( 45 1 46 8 47 6 48 7 49 ~ 50 1 51 9 52 1 53 6 54 ) 55 의 56   57 대 58 표 59 작 60   61 ‘ 62 마 63 음 64 ’ 65 에 66   67 담 68 긴 69   70 군 71 국 72 주 73 의 74 적 75   76 요 77 소 78 , 79   80 야 81 스 82 쿠 83 니 84   85 신 86 사 87   88 참 89 배 90   91 행 92 위 93 까 94 지 95   96 소 97 설 98 의 99   100 삽 101 화 102 로 103   104 동 105 원 106 하 107 며 108   109 일 110 본 111   112 사 113 회 114 의 115   116 ‘ 117 비 118 정 119 상 120 성 121 ’ 122 을 123   124 문 125 제 126   127 삼 128 는 129 다 130 . "
     ]
    }
   ],
   "source": [
    "for i, x in enumerate(text):\n",
    "    print(i, x, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp_tokens[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('오에', (0, 2)),\n",
       " ('겐자부로는', (3, 8)),\n",
       " ('일본', (9, 11)),\n",
       " ('현대문학의', (12, 17)),\n",
       " ('초석을', (18, 21)),\n",
       " ('놓은', (22, 24)),\n",
       " ('것으로', (25, 28)),\n",
       " ('평가받는', (29, 33)),\n",
       " ('작가', (34, 36)),\n",
       " ('나쓰메', (37, 40)),\n",
       " ('소세키(1867~1916)의', (41, 56)),\n",
       " ('대표작', (57, 60)),\n",
       " ('‘마음’에', (61, 66)),\n",
       " ('담긴', (67, 69)),\n",
       " ('군국주의적', (70, 75)),\n",
       " ('요소,', (76, 79)),\n",
       " ('야스쿠니', (80, 84)),\n",
       " ('신사', (85, 87)),\n",
       " ('참배', (88, 90)),\n",
       " ('행위까지', (91, 95)),\n",
       " ('소설의', (96, 99)),\n",
       " ('삽화로', (100, 103)),\n",
       " ('동원하며', (104, 108)),\n",
       " ('일본', (109, 111)),\n",
       " ('사회의', (112, 115)),\n",
       " ('‘비정상성’을', (116, 123)),\n",
       " ('문제', (124, 126)),\n",
       " ('삼는다.', (127, 131))]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
